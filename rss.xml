<?xml version="1.0" encoding="utf-8"?>






<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>CZ&#39;s Selfpage</title>
        <link>https://zchencn.github.io/</link>
        <description>Notes aboust mathematics and compter science</description>
        <generator>Hugo 0.89.4 https://gohugo.io/</generator>
        
            <language>en</language>
        
        
            <managingEditor>zchen@lsec.ac.cc.cn (zchen)</managingEditor>
        
        
            <webMaster>zchen@lsec.ac.cc.cn (zchen)</webMaster>
        
        
            <copyright>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</copyright>
        
        <lastBuildDate>Tue, 09 May 2023 21:00:37 &#43;0800</lastBuildDate>
        
            <atom:link rel="self" type="application/rss&#43;xml" href="https://zchencn.github.io/rss.xml" />
        
        
            <item>
                <title>Derivative of Trace and Determinant</title>
                <link>https://zchencn.github.io/post/derivativetracedeterminant/</link>
                <guid isPermaLink="true">https://zchencn.github.io/post/derivativetracedeterminant/</guid>
                <pubDate>Wed, 02 Mar 2022 23:05:12 &#43;0800</pubDate>
                
                    <author>zchen@lsec.ac.cc.cn (zchen)</author>
                
                <copyright>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</copyright>
                
                    <description>&lt;p&gt;The derivative of trace or determinant with respect to the matrix is vital when calculating the derivate of lagrangian in matrix optimization problems and finding the  maximum likelihood estimation of multivariate gaussian distribution.&lt;/p&gt;
&lt;h2 id=&#34;matrix-valued-derivative&#34;&gt;Matrix-Valued Derivative&lt;/h2&gt;
&lt;p&gt;Let $f$ be a scaler function of a matrix $X \in \mathbb{R}^{n\times n}$, the derivative of it with respect to $X$ can be defined as following
$$
\left(\frac{\partial f}{\partial X}\right)_{ij} = \frac{\partial f}{\partial X_{ij}}
$$
That is to say, the derivative is a matrix, the element of which is the derivative with respect to the element of matrix $X$.&lt;/p&gt;
&lt;h2 id=&#34;derivative-of-trace&#34;&gt;Derivative of Trace&lt;/h2&gt;
&lt;p&gt;Now, let $f$ be the trace of $X$
$$
f(X) = tr(X) = \sum_{i}X_{ii}
$$
Then, it&amp;rsquo;s easy to find that
$$
\frac{\partial f}{\partial X_{ii}} = 1
$$
We can write in the matrix form
$$
\frac{\partial tr(X)}{\partial X} = I
$$
Moreover,
$$
f(X) = tr(AXB) = \sum_{ijk}A_{ij}X_{jk}B_{ki}
$$
Then
$$
\frac{\partial f}{\partial X_{jk}} = \sum_{i}A_{ij}B_{ki} = (BA)_{kj} = (A^TB^T)_{jk}
$$
So that
$$
\frac{\partial tr(AXB)}{\partial X} = A^TB^T
$$
Similarly, if $f$ be the trace of square of the matrix, then
$$
f(X) = tr(X^2) = \sum_{i}(X^2)_{ii} = \sum_{ik}X_{ik}X_{ki} = \sum_{i}X^2_{ii} + \sum_{k&amp;gt;i}2X_{ik}X_{ki}
$$
so
$$
\frac{\partial tr(X^2)}{\partial X_{ik}} = 2X_{ki}
$$
Thus, we have
$$
\frac{\partial tr(X^2)}{\partial X} = 2X^T
$$&lt;/p&gt;
&lt;h2 id=&#34;derivative-of-determinant&#34;&gt;Derivative of Determinant&lt;/h2&gt;
&lt;p&gt;The determinant of a matrix is complicated to expressed as the summation of its elements,  however, from the Laplace expansion, also known as cofactor expansion, we have
$$
det(X) = \sum_{j}(-1)^{i+j}X_{ij}M_{ij}
$$
so
$$
\frac{\partial det(X)}{\partial X_{ij}} = (-1)^{i+j}M_{ij} = (adj(A)^T)_{ij}
$$
where $adj(A)$ is the adjugate matrix of $A$, so
$$
\frac{\partial det(X)}{\partial X} = adj(A)^T = det(A^T)A^{-T} = det(A)A^{-T}
$$&lt;/p&gt;
&lt;h2 id=&#34;maximum-likelihood-estimation&#34;&gt;Maximum Likelihood Estimation&lt;/h2&gt;
&lt;p&gt;Given a data set ${x_1, x_2, \cdots, x_n}$ sampled from a multivariate Gaussian distribution. We want to estimate the parameters of the distribution via maximum likelihood.&lt;/p&gt;
&lt;p&gt;The probability density function of multivariate Gaussian distribution is
$$
f(x; \mu, \Sigma) = \frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}}e^{\frac{-(x-\mu)\Sigma^{-1}(x-\mu)^T}{2}}
$$
We form the log likelihood function by taking the logarithm of product of $n$ Gaussian distributions:
$$
l(x; \mu, \Sigma) = -\frac{nd}{2}\log{2\pi} - \frac{n}{2}\log{|\Sigma|} - \frac{1}{2}\prod_{i=1}^n(x_i-\mu)\Sigma^{-1}(x_i-\mu)^T
$$
We need to take the derivative with respect to $\Sigma$ an set to zero.&lt;/p&gt;
&lt;p&gt;As previously mentioned,
$$
\frac{\partial \log{|\Sigma|}}{\partial \Sigma^{-1}} = \frac{1}{|\Sigma|}\frac{\partial |\Sigma|}{\partial \Sigma^{-1}} = \frac{1}{|\Sigma|}\frac{\partial}{\partial \Sigma^{-1}}\frac{1}{|\Sigma^{-1}|} = -\Sigma^T
$$
and
$$
\frac{\partial x\Sigma x^T}{\partial \Sigma} = \frac{\partial tr( x\Sigma x^T)}{\partial \Sigma} = \frac{\partial tr( xx^T\Sigma)}{\partial \Sigma} = (xx^T)^T=xx^T
$$
so
$$
\frac{\partial l}{\partial \Sigma^{-1}} = \frac{n}{2}\Sigma^T - \frac{1}{2}\prod_{i=1}^n(x_i-\mu)(x_i-\mu)^T
$$
Finally, setting to zero yield the maximum likelihood estimator:
$$
\Sigma_{ML} = \frac{1}{n}\prod_{i=1}^n(x_i-\mu)(x_i-\mu)^T
$$&lt;/p&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://paulklein.ca/newsite/teaching/matrix%20calculus.pdf&#34;&gt;Matrix Calculus - Notes on the Derivative of a Trace&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Laplace_expansion&#34;&gt;Laplace expansion - Wikipedia&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Adjugate_matrix&#34;&gt;Adjugate matrix - Wikipedia&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/other-readings/chapter13.pdf&#34;&gt;The Multivariate Gaussian&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
                
                
                
                
                
                    
                        
                            
                                
                                
                                
                                    <category domain="https://zchencn.github.io/categories/math/">Math</category>
                                
                            
                        
                    
                        
                            
                                
                                
                                
                                    <category domain="https://zchencn.github.io/tags/mle/">MLE</category>
                                
                            
                                
                                
                                
                                    <category domain="https://zchencn.github.io/tags/derivative/">Derivative</category>
                                
                            
                        
                    
                
            </item>
        
            <item>
                <title>求解非齐次偏微分方程的方法</title>
                <link>https://zchencn.github.io/post/nonhomogeneous/</link>
                <guid isPermaLink="true">https://zchencn.github.io/post/nonhomogeneous/</guid>
                <pubDate>Sun, 19 Dec 2021 20:22:08 &#43;0800</pubDate>
                
                    <author>zchen@lsec.ac.cc.cn (zchen)</author>
                
                <copyright>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</copyright>
                
                    <description>&lt;p&gt;利用&lt;strong&gt;分离变量法&lt;/strong&gt;可以求解具有齐次边界条件的齐次波动方程、热传导方程和拉普拉斯方程。接下来我们讨论如何处理非齐次方程和非齐次边界条件的情况。我们将介绍&lt;strong&gt;齐次化原理&lt;/strong&gt;和&lt;strong&gt;本征函数法&lt;/strong&gt;两种方法用来求解带有齐次边界条件的非齐次方程，再介绍通过构造辅助函数的方法将非齐次边界条件的问题，转化为求解非齐次方程的问题。&lt;/p&gt;
&lt;h2 id=&#34;齐次化原理&#34;&gt;齐次化原理&lt;/h2&gt;
&lt;p&gt;先考虑非齐次方程的情况。我们可以使用齐次化原理求解。回忆微积分课程中学过的积分微商定理，设
$$
U(x) = \int_a^xf(x, \tau)d\tau
$$
$a$ 是一个常数，则有
$$
\frac{d}{dx}U(x) = f(x, x) + \int_a^x\frac{\partial }{\partial x}f(x, \tau)d\tau
$$
考虑以下无界弦的强迫振动的定界问题
$$
\begin{equation}
\begin{cases}
\frac{\partial^2u}{\partial t^2} = a^2\frac{\partial^2u}{\partial x^2} + f(x, t), \quad -\infty&amp;lt;x&amp;lt;\infty, t&amp;gt;0\\
u(x, 0) = \frac{\partial}{\partial t}u(x, 0) = 0, \quad -\infty &amp;lt; x &amp;lt; \infty
\end{cases}
\end{equation}
$$
设 $\Omega(x, t, \tau)$ 是如下齐次方程的解
$$
\begin{equation}
\begin{cases}
\frac{\partial^2\Omega}{\partial t^2} = a^2\frac{\partial^2\Omega}{\partial x^2}, \quad -\infty&amp;lt;x&amp;lt;\infty, t&amp;gt;\tau\\
\Omega(x, \tau) =0,  \frac{\partial}{\partial t}\Omega(x, \tau) = f(x, \tau), \quad -\infty &amp;lt; x &amp;lt; \infty
\end{cases}
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;令
$$
u(x, t) = \int_0^t\Omega(x, t, \tau)d\tau
$$
容易发现
$$
u(x, 0) = \int_0^0\Omega(x, t, \tau)d\tau = 0
$$
利用前面的引理和 $\Omega$ 方程的初始条件可以验证
$$
\frac{\partial u}{\partial t}(x, 0) = \Omega(x, t, t) + \Big[\int_0^t\frac{\partial}{\partial t}\Omega(x, t, \tau)d\tau\Big]_{t=0} = 0
$$
最后对上式在求一次时间的导数，同样有
$$
\begin{equation}
\begin{aligned}
\frac{\partial^2u}{\partial t^2} &amp;amp;= \Big[\frac{\partial \Omega}{\partial t}(x, t, \tau)\Big]_{\tau=t} + \int_0^t\frac{\partial^2}{\partial t^2}\Omega(x, t, \tau)d\tau\\
&amp;amp;= f(x, t) + \int_0^ta^2\frac{\partial^2}{\partial x^2}\Omega(x, t, \tau)d\tau\\
&amp;amp;= f(x, t) + a^2\frac{\partial^2}{\partial x^2}\int_0^t\Omega(x, t, \tau)d\tau\\
&amp;amp;= f(x, t) + a^2\frac{\partial^2}{\partial x^2}u(x, t)
\end{aligned}
\end{equation}
$$
也即 $u$ 是原来非齐次方程的解。以上就是所谓齐次化原理，也叫&lt;strong&gt;冲量原理&lt;/strong&gt;或者&lt;strong&gt;Duhamel&amp;rsquo;s Principle&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;本征函数法&#34;&gt;本征函数法&lt;/h2&gt;
&lt;p&gt;第二种要介绍的方法是本征函数法。利用分离变量法需要使用叠加原理，这要求方程必须是齐次的且有齐次边界条件。在处理非齐次方程时，直接根据齐次的边界条件选取本征值写出级数形式的解，再利用方程和初始条件来确定技术展开中的系数，这种方法就是本征函数法。考虑两端固定的弦振动问题
$$
\begin{equation}
\begin{cases}
\frac{\partial^2u}{\partial t^2} = a^2\frac{\partial^2u}{\partial x^2} + f(x, t), \quad 0&amp;lt;x&amp;lt;l, t&amp;gt;0\\
u(x, 0) = \frac{\partial}{\partial t}u(x, 0) = 0, \quad 0 &amp;lt; x &amp;lt; l\\
u(0, t) = u(l, t) = 0
\end{cases}
\end{equation}
$$
本征函数集由对应齐次方程和边界条件给出，所以容易知道对应级数形式的解为
$$
u(x,t) = \sum_{n\geq 1}g_n(t)\sin\frac{n\pi}{l}x
$$&lt;/p&gt;
&lt;p&gt;其中
$$
g_n(t) = \frac{2}{l}\int_0^lu(x, t)\sin\frac{n\pi}{l}xdx
$$
所以容易有
$$
g_n(0) = g^{&#39;}_n(0) = 0
$$&lt;/p&gt;
&lt;p&gt;带入方程得到
$$
\sum_{t\geq1}\Big[g^{&#39;&#39;}_n(t) + \Big(\frac{na\pi}{l}\Big)^2g_n(t)\Big]\sin\frac{n\pi}{l}x = f(x, t)
$$
对 $f$ 做半幅傅里叶级数展开
$$
f(x, t) = \sum_{t\geq 1}f_n(t)\sin\frac{n\pi}{l}x\
f_n(t) = \frac{2}{l}\int_0^lf(x, t)\sin\frac{n\pi}{l}xdx
$$
对比可得关于 $g_n(t)$ 的二阶常系数非齐次微分方程
$$
g^{&#39;&#39;}_n + \Big(\frac{na\pi}{l}\Big)^2g_n - f_n(t) = 0
$$
两边对时间作拉普拉斯变换
$$
p^2G_n(p) +  \Big(\frac{na\pi}{l}\Big)^2G_n(p) - F_n(p) = 0
$$
得到
$$
G_n(p) = \frac{F_n(p)}{p^2 +  \Big(\frac{na\pi}{l}\Big)^2}
$$
利用拉普拉斯变换卷积的性质和平移性质以及 $\sin t$ 的拉普拉斯变化，容易看出
$$
g_n(t) = \frac{l}{na\pi}\int_0^tf_n(\tau)\sin \frac{na\pi}{l}(t-\tau)d\tau
$$
代入级数展开我们就得到了原非齐次方程的解。&lt;/p&gt;
&lt;h2 id=&#34;非齐次边界条件&#34;&gt;非齐次边界条件&lt;/h2&gt;
&lt;p&gt;以上处理的都是齐次边界的情况，接下来考虑方程有如下非齐次边界条件
$$
u(0, t) = \alpha(t), \quad u(l, t) = \beta(t)
$$
引入辅助函数
$$
\gamma(x, t) = (1 - \frac{x}{l})\alpha(t) + \frac{x}{l}\beta(t)
$$
设
$$
v(x, t) = u(x, t) - \gamma(x, t)
$$
那么容易验证 $v$ 满足齐次边界条件且
$$
\begin{aligned}
\frac{\partial^2v}{\partial t^2} - a^2\frac{\partial^2v}{\partial x^2} &amp;amp;= \frac{\partial^2u}{\partial t^2} - \frac{\partial^2\gamma}{\partial t^2} - a^2\Big(\frac{\partial^2u}{\partial x^2}-\frac{\partial^2\gamma}{\partial x^2}\Big)\\
&amp;amp;= f(x, t) - \frac{\partial^2\gamma}{\partial t^2} + a^2\frac{\partial^2\gamma}{\partial x^2}
\end{aligned}
$$
这样我们把非齐次边界问题转化为了带齐次边界条件的非齐次方程的问题，前面我们已经介绍过了。&lt;/p&gt;
</description>
                
                
                
                
                
                    
                        
                            
                                
                                
                                
                                    <category domain="https://zchencn.github.io/categories/math/">Math</category>
                                
                            
                                
                                
                                
                                    <category domain="https://zchencn.github.io/categories/pde/">PDE</category>
                                
                            
                        
                    
                        
                            
                                
                                
                                
                                    <category domain="https://zchencn.github.io/tags/math/">Math</category>
                                
                            
                                
                                
                                
                                    <category domain="https://zchencn.github.io/tags/pde/">PDE</category>
                                
                            
                        
                    
                
            </item>
        
            <item>
                <title>CrossValidation</title>
                <link>https://zchencn.github.io/post/crossvalidation/</link>
                <guid isPermaLink="true">https://zchencn.github.io/post/crossvalidation/</guid>
                <pubDate>Thu, 02 Dec 2021 21:40:00 &#43;0800</pubDate>
                
                    <author>zchen@lsec.ac.cc.cn (zchen)</author>
                
                <copyright>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</copyright>
                
                    <description>&lt;p&gt;Apart from the parameters that will be adjusted during the process of training, there exits some parameters, that are not learned and have to be configured by ourselves in advance. These parameters are also referred to as &lt;strong&gt;hyper-parameters&lt;/strong&gt; which have immense impact on the final results.  Given a set of hyper-parameters, we have to assess our final trained machine learning model while testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data. This situation is called &lt;strong&gt;overfitting&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;One way to overcome this problem is to not use the entire data set when training a learner. Some of the data is removed before training begins. Then when training is done, the data that was removed can be used to test the performance of the learned model on new data. This is the basic idea for a whole class of model evaluation methods called &lt;strong&gt;cross validation&lt;/strong&gt; which allows us to compare different machine learning models and get a sense of how well they will work in practice.  The best parameters can be determined by grid search.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://scikit-learn.org/stable/_images/grid_search_workflow.png&#34; alt=&#34;Grid search workflow&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;holdout-method&#34;&gt;Holdout Method&lt;/h2&gt;
&lt;p&gt;The &lt;strong&gt;holdout method&lt;/strong&gt; is the simplest kind of cross validation. The data set is separated into two sets, called the training set and the testing set. The training process using the training set only., then the model  is asked to predict the output values for the data in the testing set that never seen before. The errors on the testing data set is used to evaluate the model. The advantage of this method is that it is usually preferable to the residual method and takes no longer to compute. However, its evaluation can have a high variance. The evaluation may depend heavily on which data points end up in the training set and which end up in the test set, and thus the evaluation may be significantly different depending on how the division is made.&lt;/p&gt;
&lt;h2 id=&#34;k-fold-cross-validation&#34;&gt;K-fold Cross Validation&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;K-fold cross validation&lt;/strong&gt; is one way to improve over the holdout method. The data set is divided into &lt;em&gt;k&lt;/em&gt; subsets, and the holdout method is repeated &lt;em&gt;k&lt;/em&gt; times. Each time, one of the &lt;em&gt;k&lt;/em&gt; subsets is used as the test set and the other &lt;em&gt;k-1&lt;/em&gt; subsets are put together to form a training set. Then the average error across all &lt;em&gt;k&lt;/em&gt; trials is computed. The advantage of this method is that it matters less how the data gets divided. Every data point gets to be in a test set exactly once, and gets to be in a training set &lt;em&gt;k-1&lt;/em&gt; times. The variance of the resulting estimate is reduced as &lt;em&gt;k&lt;/em&gt; is increased. The disadvantage of this method is that the training algorithm has to be rerun from scratch &lt;em&gt;k&lt;/em&gt; times, which means it takes &lt;em&gt;k&lt;/em&gt; times as much computation to make an evaluation. A variant of this method is to randomly divide the data into a test and training set &lt;em&gt;k&lt;/em&gt; different times. The advantage of doing this is that you can independently choose how large each test set is and how many trials you average over.&lt;/p&gt;
&lt;h2 id=&#34;leave-one-out-cross-validation&#34;&gt;Leave-one-out Cross Validation&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;leave-one-out cross validation&lt;/strong&gt; is K-fold cross validation taken to its logical extreme, with K equal to N, the number of data points in the set. That means that N separate times, the model is trained on all the data except for one point and a prediction is made for that point. As before the average error is computed and used to evaluate the model. The evaluation given by leave-one-out cross validation error  is good, but at first pass it seems very expensive to compute.&lt;/p&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cs.cmu.edu/~schneide/tut5/node42.html&#34;&gt;Cross Validation (cmu.edu)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://scikit-learn.org/stable/modules/cross_validation.html&#34;&gt;3.1. Cross-validation: evaluating estimator performance — scikit-learn 1.0.1 documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Cross-validation_(statistics)&#34;&gt;Cross-validation (statistics) - Wikipedia&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
                
                
                
                
                
                    
                        
                            
                                
                                
                                
                                    <category domain="https://zchencn.github.io/categories/machine-learning/">Machine Learning</category>
                                
                            
                        
                    
                        
                            
                                
                                
                                
                                    <category domain="https://zchencn.github.io/tags/model-selection/">Model Selection</category>
                                
                            
                                
                                
                                
                                    <category domain="https://zchencn.github.io/tags/machine-learning/">Machine Learning</category>
                                
                            
                                
                                
                                
                                    <category domain="https://zchencn.github.io/tags/cross-validation/">Cross Validation</category>
                                
                            
                        
                    
                
            </item>
        
            <item>
                <title>Linear Regression</title>
                <link>https://zchencn.github.io/post/linearregression/</link>
                <guid isPermaLink="true">https://zchencn.github.io/post/linearregression/</guid>
                <pubDate>Fri, 26 Nov 2021 00:00:00 &#43;0000</pubDate>
                
                    <author>zchen@lsec.ac.cc.cn (zchen)</author>
                
                <copyright>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</copyright>
                
                    <description>&lt;h2 id=&#34;mathematic-model&#34;&gt;Mathematic model&lt;/h2&gt;
&lt;p&gt;We assume a sample $x$ has n features $x_i(i = 1, 2, \cdots, n),$ and the goal is to minimize the cost funtion:
$$J_{\theta} = \frac{1}{2m}\sum_{i=1}^{m}[\theta_0+\theta_1x_1^{(i)}+\cdots+\theta_nx_n^{(i)}-y^{(i)}]^2$$
$m$ is the number of sample, the supscript $i$ represent the $i-th$ sample, $y^{(i)}$ is the label of each sample and $\theta=(\theta_0, \theta_1, \cdots, \theta_n)^T$ is the parameter of cost function $J$.&lt;/p&gt;
&lt;h2 id=&#34;gradient-desent&#34;&gt;Gradient desent&lt;/h2&gt;
&lt;h3 id=&#34;algorithm&#34;&gt;Algorithm&lt;/h3&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Step1: Choose a start point $\theta$;&lt;/li&gt;
&lt;li&gt;Step2: Take a step to the direction of minus gradient with the step size of $\alpha$, i.e $\theta_{new} = \theta_{old} - \alpha\nabla J$;&lt;/li&gt;
&lt;li&gt;Step3: Repeat step2 until $J_{\theta}(x)$ is samll enough;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;data-preprocessing&#34;&gt;Data preprocessing&lt;/h3&gt;
&lt;p&gt;To accelerate the speed of convergence, some preprocessing can be applied the data set.&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Feature scaling: $\frac{x_i}{r},$ $r = x_i^{max}-x_i^{min}$ is the range value of feature $i$;&lt;/li&gt;
&lt;li&gt;Mean normalization: $x_i-\mu_i, \mu_i$ is the mean value of feature $i$;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;learning-rate&#34;&gt;Learning rate&lt;/h3&gt;
&lt;p&gt;We call the step size $\alpha$ learnig rate.&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;If $\alpha$ is too large, the cost function may not decrease on every  iteration, and  the algorithm may not converge;&lt;/li&gt;
&lt;li&gt;If $\alpha$ is too small, the algorithm may converge slowly;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;feature-and-polynomial-regression&#34;&gt;Feature and polynomial regression&lt;/h3&gt;
&lt;p&gt;We can create the new feature from the existing feature, for example $x_i^n$. Then it become a polynomial regression problem, but we can solve it by the same methods of linear regression.&lt;/p&gt;
&lt;h2 id=&#34;normal-equation&#34;&gt;Normal equation&lt;/h2&gt;
&lt;h3 id=&#34;matrix-form&#34;&gt;Matrix form&lt;/h3&gt;
&lt;p&gt;The problem can be expressed in the matrix form, let $x=(x_0,x_1,\cdots,x_n)^T,\ x_0=1,\ X^T=(x^{(1)},x^{(2)},\cdots,x^{(n)}),Y=(y^{(1)},y^{(2)},\cdots,y^{(n)})^T,$ then:
$$J_{\theta}=\frac{1}{2m}||X\theta-Y||^2=\frac{1}{2m}(X\theta-Y)^T(X\theta-Y)$$
$||\cdot||$ is the Euclidean norm, the problem become:
$$\min_{\theta}J_{\theta}$$
$J$ is a convex function, so the local minimum is also the unique global minimum, and the stationary point is the very global minimum:
$$\arg\min_{\theta}J_{\theta}=(X^TX)^{-1}X^TY$$&lt;/p&gt;
&lt;h3 id=&#34;comparision&#34;&gt;Comparision&lt;/h3&gt;
&lt;p&gt;If we use normal equation to solve regression problem, feature scaling is not necesssary, but when the data set is too large we tend to use gradient desent rather than solving normal equation.&lt;br&gt;
If $X^TX$ is non-invertible we can delete some features, use pseudoinverse or use regulization which we will talk about later.&lt;/p&gt;
</description>
                
                
                
                
                
                    
                        
                            
                                
                                
                                
                                    <category domain="https://zchencn.github.io/categories/machine-learning/">Machine Learning</category>
                                
                            
                                
                                
                                
                                    <category domain="https://zchencn.github.io/categories/regression/">Regression</category>
                                
                            
                        
                    
                        
                            
                                
                                
                                
                                    <category domain="https://zchencn.github.io/tags/regression/">Regression</category>
                                
                            
                        
                    
                
            </item>
        
            <item>
                <title>An Introduction to Optimal Transport</title>
                <link>https://zchencn.github.io/post/sinkhorn/</link>
                <guid isPermaLink="true">https://zchencn.github.io/post/sinkhorn/</guid>
                <pubDate>Sun, 31 Oct 2021 16:38:57 &#43;0800</pubDate>
                
                    <author>zchen@lsec.ac.cc.cn (zchen)</author>
                
                <copyright>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</copyright>
                
                    <description>&lt;h2 id=&#34;optimal-transport-overview&#34;&gt;Optimal Transport Overview&lt;/h2&gt;
&lt;p&gt;Optimal transportation (OT) problem was first study by Gaspard Monge in 1781: A worker with a shovel in hand want to move a large pile of sand lying on a construction site and wish to minimize her total effort.  OT arouse the interest of mathematicians because it can compare two probability distribution. OT has been rediscovered in many settings and under different forms, giving it a rich history. Kantorovich in 1940s established its significance to logistics and economics. Dantzig solved it numerically in 1949 within the framework of linear programming, giving OT a firm footing in optimization. In recent years, thanks to the emergence of approximate solvers that can scale to large problem dimension, OT is being increasingly used to unlock various problems in imaging sciences (such as color or texture processing), graphics (for shape manipulation) or machine learning (for regression, classification and generative modeling).&lt;/p&gt;
&lt;p&gt;We mainly focus on the numerical aspect of OT, for more theoretical detail, please reference the work of Villani.&lt;/p&gt;
&lt;h2 id=&#34;monge-problem&#34;&gt;Monge Problem&lt;/h2&gt;
&lt;p&gt;We say $\textbf{a}$ is an histogram or probability vector if  it belongs to the probability simplex:
$$
\Sigma_n = \Big\{\textbf{a}\in\mathbb{R}^n_{+}: \sum_{i=1}^na_i = 1\Big\}
$$
That is to say, the elements of $\textbf{a}$ is nonnegative and the sum of them is one. A discrete measure with weights $\textbf{a}$ and locations $x_1, x_2, \cdots, x_n$ reads:
$$
\alpha = \sum_{i=1}^na_i\delta_{x_i}
$$
where $\delta_x$ is the Dirac at  position $x$  intuitively a unit of mass which is infinitely concentrated at location $x$.  For discrete measure:
$$
\alpha = \sum_{i=1}^na_i\delta_{x_i}\quad \text{and}\quad\beta = \sum_{i=1}^mb_i\delta_{y_i}
$$
the Monge problem seeks a map the associates to each point $x_i$ a single point $y_i$ and which must push the mass of $\alpha$ toward the mass of $\beta$, namely, such a map $T:\{x_1, \cdots, x_n\} \rightarrow \{y_1, \cdots, y_m\}$ must verify that:
$$
\forall j \in {1, 2, \cdots, m}, b_j = \sum_{T(x_i)=y_j}a_i
$$
which we write in compact form as:
$$
T_{\sharp}\alpha = \beta
$$
Given a cost function $c(x, y)$, the Monge problem is to find the map that minimize the total cost of the transportation:
$$
\min_T\Big\{\sum_{i}c(x_i, T(x_i)): T_{\sharp}\alpha=\beta\Big\}
$$
Monge maps may not even exist between a discrete measure to another.&lt;/p&gt;
&lt;h2 id=&#34;kantorovich-relaxation&#34;&gt;Kantorovich Relaxation&lt;/h2&gt;
&lt;p&gt;The key idea of Kantorovich is to relax the deterministic nature of transportation, namely the fact that a source point $x_i$ can only be assigned to another point or location $T(x_i)$ only.  Kantorovich proposed instead that the mass at any point $x_i$ be potentially dispatched across several locations. This flexibility is encoded using a coupling matrix $P \in \mathbb{R}^{n\times m}_+$, where $P_{ij}$ describes the amount of mass flowing from bin $i$ to bin $j$.  Admissible couplings admit a simple characterization that:
$$
U(\textbf{a}, \textbf{b}) = \Big\{P\in\mathbb{R}^{n\times m}_+: \sum_jP_{ij}=a_i, \sum_iP_{ij}=b_j\Big\}
$$
The set of matrices $U(\textbf{a}, \textbf{b})$ is bounded and defined by $n+m$ equality constraints, and therefore is a convex polytope.&lt;/p&gt;
&lt;p&gt;Given a cost matrix $C$, Kantorovich&amp;rsquo;s OT problem now reads:
$$
L_C(\textbf{a}, \textbf{b}) = \min_{P\in U(\textbf{a}, \textbf{b})}\Big&amp;lt;P, C\Big&amp;gt; = \sum_{i,j}C_{ij}P_{ij}
$$
This is a linear program and as is usually the case with such programs, its optimal solutions are not necessarily unique.&lt;/p&gt;
&lt;h2 id=&#34;wasserstein-distance&#34;&gt;Wasserstein Distance&lt;/h2&gt;
&lt;p&gt;An import feature if OT is that it defines a distance between histograms and probability measures as soon as the cost matrix satisfies certain suitable properties. We suppose $n=m$ and that for some $p\geq 1$, $C = D^p$ where $D\in \mathbb{R}^{n\times n}_+$ is a distance matrix, that is to say $D$ satisfy following properties:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$D$ is symmetric&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$D_{ij} = 0$ if and only if $i=j$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$\forall i, j, k, D_{ik} \leq D_{ij} + D_{jk}$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then we can define so-called Wasserstein distance on probability simplex $\Sigma_n$,
$$
W_p(\textbf{a}, \textbf{b}) = L_{D^p}(\textbf{a}, \textbf{b})^{1/p}
$$&lt;/p&gt;
&lt;h2 id=&#34;entropic-regularization&#34;&gt;Entropic Regularization&lt;/h2&gt;
&lt;p&gt;We will introduce a family of numerical schemes to approximate solutions to Kantorovich formulation of OT. It operates by adding an entropic regularization to the original problem. The minimization of the regularized problem can be solved by using a simple alternate minimization scheme which are iterations of simple matrix-vector products. The resulting approximate distance is smooth with respect to input histogram weights and can be differentiated using automatic differentiation.&lt;/p&gt;
&lt;p&gt;The discrete entropy of the coupling matrix is defined as:
$$
H(P) = -\sum_{i, j}P_{ij}(\log(P_{ij})-1)
$$
The idea of the entropic regularization of OT is to use $-H$ as a regularization function to obtain approximate solutions to the origin Kantorovich OT problem:
$$
L^{\epsilon}_C(\textbf{a, b}, \textbf{b}) = \min_{P\in U(\textbf{a}, \textbf{b})}\Big&amp;lt;P, C\Big&amp;gt; - \epsilon H(P)
$$
Since the objective is an $\epsilon-strongly$ convex function, the problem mentioned above has a unique optimal solution.&lt;/p&gt;
&lt;p&gt;It has been proved that:
$$
L^{\epsilon}_C(\textbf{a}, \textbf{b})\stackrel{\epsilon\rightarrow0}{\longrightarrow}L_C(\textbf{a}, \textbf{b})
$$&lt;/p&gt;
&lt;h2 id=&#34;sinkhorns-algorithm&#34;&gt;Sinkhorn&amp;rsquo;s Algorithm&lt;/h2&gt;
&lt;p&gt;Let $K$ denote the Gibbs kernel associated to the cost matrix $C$ as:
$$
K_{ij} = e^{-\frac{C_{ij}}{\epsilon}}
$$
The solution of the regularized OT problem has the form:
$$
P_{ij} = u_iK_{ij}v_j
$$
for two (unknow) scaling variable $(\textbf{u}, \textbf{v}) \in \mathbb{R}^n_+\times\mathbb{R}^m_+$.&lt;/p&gt;
&lt;p&gt;The factorization of the optimal solution can be conveniently rewritten in matrix form as:
$$
P = diag(\textbf{u})Kdiag(\textbf{v})
$$
The scaling variables must therefore satisfy the following nonlinear equations which correspond to the mass conservation constraints inherent to $U(\textbf{a}, \textbf{b})$:
$$
\textbf{u}\odot(K\textbf{v}) = \textbf{a}, \quad \textbf{v}\odot(K^T\textbf{u}) = \textbf{b}
$$
where $\odot$ corresponds to entrywise multiplication of vectors. That problem is known as matrix scaling problem which can be solved iteratively by modifying first $\textbf{u}$ so that it satisfies the left-hand side of above equations and then $\textbf{v}$ to satisfy its right-hand side. These two updates define Sinkhorn&amp;rsquo;s algorithm:
$$
\textbf{u}^{(l+1)} = \frac{\textbf{a}}{K\textbf{v}^(l)}, \quad \textbf{v}^{(l+1)} = \frac{\textbf{b}}{K^T\textbf{u}^{(l+1)}}
$$
initialized with an arbitrary positive vector $\textbf{v}^{(0)} = \mathbb{1}_m$. The division operator used above between two vectors is to be understood entrywise.&lt;/p&gt;
&lt;p&gt;In order to speed up the Sinkhorn&amp;rsquo;s iterations, we can compute several regularized Wasserstein distances between pairs of histograms simultaneously. Let $N$ be an integer, $\textbf{a}_1, \cdots, \textbf{a}_N$ be histograms in $\Sigma_n$, and $\textbf{b}_1, \cdots, \textbf{b}_N$ be histograms in $\Sigma_m$. We seek to compute all $N$ approximate distances $L_C^{\epsilon}(\textbf{a}_1, \textbf{b}_1), \cdots, L_C^{\epsilon}(\textbf{a}_N, \textbf{b}_N).$  In that case, writing $A = [\textbf{a}_1, \cdots, \textbf{a}_N]$ and $B = [\textbf{b}_1, \cdots, \textbf{b}_N]$ for the $n\times N$ and $m\times N$ matrices storing all histograms,  one can notice that all Sinkhorn iterations for all these $N$ pairs can be carried out in parallel, by setting, for instance,
$$
\textbf{U}^{(l+1)} = \frac{\textbf{A}}{K\textbf{V}^(l)}, \quad \textbf{V}^{(l+1)} = \frac{\textbf{B}}{K^T\textbf{U}^{(l+1)}}
$$
initialized with $\textbf{V}^{(0)} = \mathbb{1}_{m\times N}.$&lt;/p&gt;
&lt;h2 id=&#34;log-domain-stabilized-sinkhorn&#34;&gt;Log-domain Stabilized Sinkhorn&lt;/h2&gt;
&lt;p&gt;The Sinkhorn algorithm suffers from numerical overflow when the regularization parameter $\epsilon$ is small compared to the entries of the cost matrix $C$. This concern can be alleviated to some extent be carrying out computations in log domain.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://optimaltransport.github.io/&#34;&gt;Computational Optimal Transport&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
                
                
                
                
                
                    
                        
                            
                                
                                
                                
                                    <category domain="https://zchencn.github.io/categories/math/">Math</category>
                                
                            
                                
                                
                                
                                    <category domain="https://zchencn.github.io/categories/optimal-transport/">Optimal Transport</category>
                                
                            
                        
                    
                        
                            
                                
                                
                                
                                    <category domain="https://zchencn.github.io/tags/optimal-transport/">Optimal Transport</category>
                                
                            
                                
                                
                                
                                    <category domain="https://zchencn.github.io/tags/machine-learning/">Machine Learning</category>
                                
                            
                        
                    
                
            </item>
        
            <item>
                <title>Bias and Variance</title>
                <link>https://zchencn.github.io/post/variancebias/</link>
                <guid isPermaLink="true">https://zchencn.github.io/post/variancebias/</guid>
                <pubDate>Wed, 08 Sep 2021 00:00:00 &#43;0000</pubDate>
                
                    <author>zchen@lsec.ac.cc.cn (zchen)</author>
                
                <copyright>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</copyright>
                
                    <description>&lt;p&gt;在监督学习中，我们使用特定的算法在给定的带标记的训练集 $D={(x_i, y_i)}_{i=1}^N$ 上训练得到一个模型 $f(x; D)$ . 泛化能力是评价一个模型好坏的重要标准，我们希望得到的模型能够在它没有见过的输入 $x$ 上有更准确的预测能力. 在模型训练时，我们总是通过特定的算法去减小训练误差，但是在实践中我们发现，更小的训练误差并不就意味着更小的泛化误差，往往随着训练误差的减小，泛化误差呈现一个先下降后上升的趋势，我们称之为&lt;strong&gt;过拟合&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bias&lt;/strong&gt; 和 &lt;strong&gt;Variance&lt;/strong&gt; 能够帮助我们去理解模型的泛化能力.  给定测试样本 $x$ ，令 $y_{D}$ 为 $x$ 在训练集中的标记，$y$ 为真实标记，则学习算法的期望预测为
$$
\bar{f}(x) = E_{D}[f(x; D)]
$$&lt;/p&gt;
&lt;h2 id=&#34;what-is-bias&#34;&gt;What Is Bias?&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Bias&lt;/strong&gt; 是衡量模型与真实标记偏差的量，数学上如下定义
$$
bias^2 = (\bar{f}(x)-y)^2
$$
通常，一个简单模型的 $bias$ 会比复杂模型的大。比如在多项式回归中，极端地，我们使用0次多项式也即常数作为模型，那么它的输出和真实标记的偏差显然比我们使用高次多项式作为模型得到的偏差要大，因为总是存在一个 $N-1$ 次多项式能够完全拟合给定的 $N$ 个数据点.&lt;/p&gt;
&lt;h2 id=&#34;what-is-variance&#34;&gt;What Is Variance?&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Variance&lt;/strong&gt; 是衡量训练集扰动对模型产生的影响的量，数学定义如下
$$
variance = E_{D}[(f(x; D)-\bar{f}(x))^2]
$$
高的 Variance 说明当训练集发生扰动时，训练得到的模型会有很大的改变，也就是说鲁棒性比较差，这是我们不希望看到的，因为训练集的标记总是和真实标记之间存在误差。通常，一个简单模型的 Variance 会比复杂模型的小。同样以上文中多项式回归为例子，用常数作为模型无论训练集的输入如何变化得到的最终模型都不会改变，但是扰动对高次多项式的影响是较大的。&lt;/p&gt;
&lt;h2 id=&#34;decomposition&#34;&gt;Decomposition&lt;/h2&gt;
&lt;p&gt;假设训练集标记和真实标记之间的误差的期望为0，方差为 $\epsilon^2$ ，即
$$
E_{D}[y-y_D] = 0, E_{D}[(y-y_D)^2] = \epsilon^2
$$
我们对期望泛化误差可以作如下分解
$$
\begin{align}
E_D[(f(x; D) - y_D)^2] &amp;amp;= E_D[(f(x; D) -\bar{f}(x) + \bar{f}(x)- y_D)^2]\\
&amp;amp;= E_D[(f(x; D)-\bar{f}(x))^2] + E_D[(\bar{f}(x)-y+y-y_D)^2] + 2E_D[(f(x; D)-\bar{f}(x))(\bar{f}(x)-y_D)]\\
&amp;amp;= variance + E_D[(\bar{f}(x)-y)^2] + E_D[(y-y_D)^2] + 2E_D[(\bar{f}(x)-y)(y-y_D)]\\
&amp;amp;= variance + bias^2 + \epsilon^2
\end{align}
$$&lt;/p&gt;
&lt;p&gt;也就是说，泛化误差可以分解为偏差、方差与噪声之和. 噪声刻画了泛化误差的下界，反应了学习问题本身的难度.&lt;/p&gt;
&lt;h2 id=&#34;trade-off&#34;&gt;Trade-off&lt;/h2&gt;
&lt;p&gt;监督学习的希望得到一个 bias 和 Variance 都较低的模型，但是一般来说，这两者是冲突的。在训练不足时，模型的拟合能力不足，训练数据扰动不足以对模型产生显著影响，此时 bias 主导了泛化误差；随着训练程度的加深，模型的拟合能力逐渐增强，训练数据的扰动能够被算法学习到，发生了过拟合，此时 Variance 主导了泛化误差. 我们需要在这两者中找到一个好的平衡点，使得得到的模型效果足够令人满意.&lt;/p&gt;
&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;《机器学习》，周志华，清华大学出版社&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.mastersindatascience.org/learning/difference-between-bias-and-variance/&#34;&gt;What Is the Difference Between Bias and Variance? (mastersindatascience.org)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff&#34;&gt;Bias–variance tradeoff - Wikipedia&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
                
                
                
                
                
                    
                        
                            
                                
                                
                                
                                    <category domain="https://zchencn.github.io/categories/machine-learning/">Machine Learning</category>
                                
                            
                        
                    
                        
                            
                                
                                
                                
                                    <category domain="https://zchencn.github.io/tags/machine-learning/">Machine Learning</category>
                                
                            
                        
                    
                
            </item>
        
            <item>
                <title>Mutable and Immutable Objects in Python</title>
                <link>https://zchencn.github.io/post/mutableandimmutableobjects/</link>
                <guid isPermaLink="true">https://zchencn.github.io/post/mutableandimmutableobjects/</guid>
                <pubDate>Thu, 21 Jan 2021 00:00:00 &#43;0000</pubDate>
                
                    <author>zchen@lsec.ac.cc.cn (zchen)</author>
                
                <copyright>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</copyright>
                
                    <description>&lt;p&gt;Python 是一门面向对象语言，Python 中包括内置类型在内的一切都是对象。Python 对象分为可变对象(Mutable Object)和不可变对象(Immutable Object).&lt;/p&gt;
&lt;h2 id=&#34;id-and-type&#34;&gt;ID and Type&lt;/h2&gt;
&lt;p&gt;Python 中的对象再被实例化时会被分配一个与内存位置相关的id, 可以用内置的&lt;code&gt;id()&lt;/code&gt;函数查看对象的id, 用&lt;code&gt;type()&lt;/code&gt;函数查看对象的类型。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;id&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;mi&#34;&gt;140279384729648&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;type&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;a&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;err&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&#34;nc&#34;&gt;int&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h2 id=&#34;mutable-and-immutable&#34;&gt;Mutable and Immutable&lt;/h2&gt;
&lt;p&gt;Python 的内置类型也分为可变和不可变两种：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;mutable:  int  float complex sting tuple&lt;/li&gt;
&lt;li&gt;immutable:  list set dict&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;看一段代码：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;9
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;id&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;id&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;id&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;id&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;id&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;id&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;首先，我们在内存中创建了一个类型为&lt;code&gt;int&lt;/code&gt;, 值为&lt;code&gt;10&lt;/code&gt;的对象，并把它赋给变量&lt;code&gt;x&lt;/code&gt; （或者说给这个对象起了一个名字&lt;code&gt;x&lt;/code&gt;）， 再把变量&lt;code&gt;x&lt;/code&gt;表示的对象与&lt;code&gt;y&lt;/code&gt;绑定（或者说起了一个别名&lt;code&gt;y&lt;/code&gt;）.  所以&lt;code&gt;x, y, 10&lt;/code&gt; 的id是相同的。然后， 因为&lt;code&gt;int&lt;/code&gt;是不可变对象，&lt;code&gt;x + 1&lt;/code&gt;会创建一个值为&lt;code&gt;11&lt;/code&gt; 类型为 &lt;code&gt;int&lt;/code&gt; 的新对象，再把这个新对象重新与 &lt;code&gt;x&lt;/code&gt; 绑定，此时 &lt;code&gt;y&lt;/code&gt; 还是 &lt;code&gt;10&lt;/code&gt; 这个对象，所以 &lt;code&gt;x, y&lt;/code&gt; 的id不相等。&lt;/p&gt;
&lt;p&gt;再看下面一段代买：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;8
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;l1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;b&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;c&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;l2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;l1&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;id&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;l1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;id&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;l2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;l1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pop&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;span class=&#34;s1&#34;&gt;&amp;#39;c&amp;#39;&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;id&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;l1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;id&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;l2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;同样我们创建一个对象并把它赋给两个变量 &lt;code&gt;l1, 12&lt;/code&gt;, 由于 &lt;code&gt;list&lt;/code&gt; 是可变对象，  &lt;code&gt;pop&lt;/code&gt; 可以直接改变它的状态，并不用重新创建一个新的 &lt;code&gt;&#39;list&lt;/code&gt;&#39; 对象，所以 &lt;code&gt;l1, l2&lt;/code&gt; 的id还是相同的。&lt;/p&gt;
&lt;p&gt;但是，不可变对象的不可变不是绝对的，看下面一个例子：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;t&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;abcd&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;append&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;t&lt;/span&gt;
&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;abcd&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;我们首先创建了一个  &lt;code&gt;tuple&lt;/code&gt;  对象，在 Python 中， &lt;code&gt;tuple&lt;/code&gt; 是不可变对象，它的第二个成员是一个可变的 &lt;code&gt;list&lt;/code&gt; 对象， 我们改变了这个 &lt;code&gt;list&lt;/code&gt; 对象，从内容上看，&lt;code&gt;tuple&lt;/code&gt; 被改变了，但是它仍然绑定的是创建时的那个 &lt;code&gt;string&lt;/code&gt; 和 &lt;code&gt;list&lt;/code&gt; ，从这个角度看，它是没有被改变的。&lt;/p&gt;
&lt;h2 id=&#34;custom-class&#34;&gt;Custom Class&lt;/h2&gt;
&lt;p&gt;用户自定义的类默认是可变的.&lt;/p&gt;
&lt;h2 id=&#34;passed-to-function&#34;&gt;Passed to Function&lt;/h2&gt;
&lt;p&gt;不严谨地说，传递可变对象类似引用传递(Call by Reference), 传递不可变对象类似值传递(Call by Value).&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;func1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;l&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;l&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;append&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
 
&lt;span class=&#34;n&#34;&gt;l&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;func&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;l&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;l&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# [0, 1, 2, 3]&lt;/span&gt;

&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;func&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;
    
&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;func2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# 5&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;</description>
                
                
                
                
                
                    
                        
                            
                                
                                
                                
                                    <category domain="https://zchencn.github.io/categories/computer-science/">Computer Science</category>
                                
                            
                                
                                
                                
                                    <category domain="https://zchencn.github.io/categories/programming-language/">Programming Language</category>
                                
                            
                                
                                
                                
                                    <category domain="https://zchencn.github.io/categories/python/">Python</category>
                                
                            
                        
                    
                        
                            
                                
                                
                                
                                    <category domain="https://zchencn.github.io/tags/python/">Python</category>
                                
                            
                        
                    
                
            </item>
        
            <item>
                <title>Different Derivatives in Mathematics</title>
                <link>https://zchencn.github.io/post/derivatives/</link>
                <guid isPermaLink="true">https://zchencn.github.io/post/derivatives/</guid>
                <pubDate>Mon, 18 Jan 2021 00:00:00 &#43;0000</pubDate>
                
                    <author>zchen@lsec.ac.cc.cn (zchen)</author>
                
                <copyright>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</copyright>
                
                    <description>&lt;p&gt;一元函数、多元函数和泛函的导数相关概念。以下均假设导数存在。&lt;/p&gt;
&lt;h2 id=&#34;一元函数&#34;&gt;一元函数&lt;/h2&gt;
&lt;p&gt;$f:\mathbb{R} \rightarrow \mathbb{R}$ 是实数域上的一元函数，函数在某点的导数是函数值在该点关于自变量的变化率，定义如下：
$$
\frac{df(x)}{dx} = \lim_{h\to0}\frac{f(x + h) - f(x)}{h} \tag{1}
$$
上式极限存在称函数 $f$ 在 $x$ 处可导，相应的可以定义映射$f^{&#39;}:x \mapsto \frac{df(x)}{dx}$ 称 $f$ 的导函数，一般记为 $f^{&#39;}(x)$ .&lt;/p&gt;
&lt;h2 id=&#34;多元函数&#34;&gt;多元函数&lt;/h2&gt;
&lt;p&gt;$f:\mathbb{R}^n \rightarrow \mathbb{R}$ 是定义在 $n$ 维欧氏空间上的函数。&lt;/p&gt;
&lt;h3 id=&#34;偏导数&#34;&gt;偏导数&lt;/h3&gt;
&lt;p&gt;对于多变量函数，如果我们固定除某一个变量外的其他变量，此时它可以看作一个单变量函数，此时该函数关于未固定这个变量的导数，就是多元函数的偏导数：
$$
\frac{\partial f(x_1, x_2, \cdots, x_n )}{\partial x_k} = \lim_{h\to0}\frac{f(x_1, \cdots, x_k + h, \cdots, x_n) - f(x_1, \cdots, x_k, \cdots, x_n)}{h}
$$&lt;/p&gt;
&lt;h3 id=&#34;梯度&#34;&gt;梯度&lt;/h3&gt;
&lt;p&gt;假设 $f$ 在 $\mathbf{p}$ 点的偏导数都存在，则可以定义它在该点的梯度向量：
$$
grad f(\mathbf{p}) = \nabla f(\mathbf{p}) =
\begin{bmatrix}
\frac{\partial f(\mathbf{p})}{\partial x_1}\\
\frac{\partial f(\mathbf{p})}{\partial x_2}\\
\vdots \\
\frac{\partial f(\mathbf{p})}{\partial x_n}
\end{bmatrix}
$$
$\nabla f: \mathbf{R}^n \rightarrow \mathbf{R}^n, \mathbf{p} \mapsto \nabla f(\mathbf{p})$ 是函数 $f$ 的梯度场。&lt;/p&gt;
&lt;h3 id=&#34;方向导数&#34;&gt;方向导数&lt;/h3&gt;
&lt;p&gt;为了衡量多变量函数在某点沿着某个方向的变化率，可以引入方向导数的概念，$f$ 在 $\mathbf{x}$ 点沿 $ \mathbf{p}$ 方向的方向导数为：
$$
\nabla_{\mathbf{v}}f(\mathbf{x}) = \lim_{h\to0}\frac{f(\mathbf{x} + h\mathbf{v}) -f(\mathbf{x})}{h}
$$
特别地，偏导数可以看作函数沿 $\mathbf{e_k}$ 方向的方向导数：
$$
\nabla_{\mathbf{e_k}}f(\mathbf{x}) = \lim_{h\to0}\frac{f(\mathbf{x} + h\mathbf{e_k}) - f(\mathbf{x})}{h} = \lim_{h\to0}\frac{f(x_1, \cdots, x_k + h, \cdots, x_n) - f(x_1, \cdots, x_k, \cdots, x_n)}{h} = \frac{\partial f(\mathbf{x})}{\partial x_k}
$$
另外：
$$
f(x_1 + h v_1, x_2 + hv_2) = f(x_1, x_2) + \frac{\partial f}{\partial x_1}hv_1 + \frac{\partial f}{\partial x_2}hv_2 + o(h)
$$
所以：
$$
\nabla_{\mathbf{v}}f(x) = \nabla f(\mathbf{x})\cdot \mathbf{v} = \nabla f(\mathbf{x})^T\mathbf{v}
$$&lt;/p&gt;
&lt;h3 id=&#34;全微分&#34;&gt;全微分&lt;/h3&gt;
&lt;p&gt;偏导数和方向导数衡量的都可以看作是函数沿着某一方向的近似，而全微分可以看作是函数在某点的最佳线性逼近，和方向无关：
$$
f(\mathbf{x} + \mathbf{h}) = f(\mathbf{x}) + df_{\mathbf{x}}(\mathbf{h}) + o(||\mathbf{h}||)
$$
$df_{\mathbf{x}}: \mathbb{R}^n \rightarrow R$ 是 $f$ 在 $\mathbf{x}$ 的线性全微分算子。&lt;/p&gt;
&lt;p&gt;另有：
$$
0 = \lim_{t\to 0}\frac{f(\mathbf{x} + t\mathbf{h}) - f(\mathbf{x}) - df_{\mathbf{x}}(t\mathbf{h})}{t} = \nabla_{\mathbf{h}}f(\mathbf{x}) - df_{\mathbf{x}}(\mathbf{h})
$$
即：$\nabla_{\mathbf{h}}f(\mathbf{x}) = df_{\mathbf{x}}(\mathbf{h}) = \nabla f(\mathbf{x})^T\mathbf{h}$.&lt;/p&gt;
&lt;p&gt;由希尔伯特空间上的Riesz表示定理， 存在 $\mathbf{p_x} \in \mathbb{R}^n$ 使得：
$$
df_{\mathbf{x}}(\mathbf{h}) = \mathbf{p_x}^T\mathbf{h}
$$
也即 $f$ 在该点的梯度，所以全微分可以看作是梯度的对偶。&lt;/p&gt;
&lt;h2 id=&#34;g导数&#34;&gt;G导数&lt;/h2&gt;
&lt;p&gt;设 $F: U\rightarrow R$ 是线性空间 $U$ 上的实泛函。从方向导数推广，可以定义泛函的G导数(Gateaux):
$$
dF(u; v) = \lim_{t \to 0}\frac{F(u + tv) - F(u)}{t}
$$&lt;/p&gt;
&lt;h2 id=&#34;f导数&#34;&gt;F导数&lt;/h2&gt;
&lt;p&gt;进一步假设 $U$ 是Banach空间，可以定义泛函的F导数(Frechet):
$$
F(u + h) = F(u) + Df_u(h) + o(||h||)
$$
$Df_u : U \rightarrow R$&lt;/p&gt;
&lt;p&gt;F可导一定G可导，G可导不一定F可导。&lt;/p&gt;
</description>
                
                
                
                
                
                    
                        
                            
                                
                                
                                
                                    <category domain="https://zchencn.github.io/categories/math/">Math</category>
                                
                            
                        
                    
                        
                            
                                
                                
                                
                                    <category domain="https://zchencn.github.io/tags/caculus/">Caculus</category>
                                
                            
                                
                                
                                
                                    <category domain="https://zchencn.github.io/tags/derivatives/">Derivatives</category>
                                
                            
                        
                    
                
            </item>
        
            <item>
                <title>Matric Caculus</title>
                <link>https://zchencn.github.io/post/matriccaculus/</link>
                <guid isPermaLink="true">https://zchencn.github.io/post/matriccaculus/</guid>
                <pubDate>Mon, 18 Jan 2021 00:00:00 &#43;0000</pubDate>
                
                    <author>zchen@lsec.ac.cc.cn (zchen)</author>
                
                <copyright>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</copyright>
                
                    <description>&lt;p&gt;Assume that all vectors are column vector, and we use numerator layout&lt;/p&gt;
&lt;h2 id=&#34;scalar-by-vector&#34;&gt;Scalar-by-Vector&lt;/h2&gt;
&lt;p&gt;$f$ is a scalar function of $\mathbf{x}\in \mathbb{R}^n$, then the gradient vector of $f$ with respect to $\mathbf{x}$ is:
$$
\frac{\partial f}{\partial \mathbf{x}}=\left[\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \cdots, \frac{\partial f}{\partial x_n} \right]
$$
then the gradient vector of f is:
$$
\nabla f = \sum_{i=1}^n\frac{\partial f}{\partial x_i}e_i = \left(\frac{\partial f}{\partial \mathbf{x}}\right)^T
$$&lt;/p&gt;
&lt;h2 id=&#34;vector-by-scalar&#34;&gt;Vector-by-Scalar&lt;/h2&gt;
&lt;p&gt;$\mathbf{y} \in\mathbb{R}^n$ is a n dimensional vector each component of  which is a scalar function of x, then:
$$
\frac{\partial \mathbf{y}}{\partial x} = \left[\frac{\partial y_1}{\partial x}, \frac{\partial y_2}{\partial x}, \cdots, \frac{\partial y_n}{\partial x} \right]^T
$$&lt;/p&gt;
&lt;h2 id=&#34;vector-by-vector&#34;&gt;Vector-by-Vector&lt;/h2&gt;
&lt;p&gt;$\mathbf{y} \in \mathbb{R}^m$ is a vector of $\mathbf{x} \in \mathbb{R}^n$ , the Jacobian matrix of it is a m-by-n matrix:
$$
\begin{equation}
\frac{\partial \mathbf{y}}{\partial \mathbf{x}} = \left[\nabla y_1, \nabla y_2, \cdots, \nabla y_m  \right]^T =
\begin{bmatrix}
\frac{\partial y_1}{\partial x_1} &amp;amp; \frac{\partial y_1}{\partial x_2} &amp;amp; \cdots &amp;amp; \frac{\partial y_1}{\partial x_n} \\
\frac{\partial y_2}{\partial x_1} &amp;amp; \frac{\partial y_2}{\partial x_2} &amp;amp; \cdots &amp;amp; \frac{\partial y_2}{\partial x_n} \\
\vdots							  &amp;amp; \vdots                            &amp;amp; \ddots &amp;amp; \vdots                            \\
\frac{\partial y_m}{\partial x_1} &amp;amp; \frac{\partial y_m}{\partial x_2} &amp;amp; \cdots &amp;amp; \frac{\partial y_m}{\partial x_1}
\end{bmatrix}
\end{equation}
$$
Let $\mathbf{y} = \mathbf{Ax}$ , $\mathbf{A}$ is independent of $\mathbf{x}$, then:
$$
\frac{\partial \mathbf{y}}{\partial \mathbf{x}} = \frac{\partial \mathbf{Ax}}{\partial \mathbf{x}} = \mathbf{A}
$$
If $y = \mathbf{x^TAx}$ ,
$$
\frac{\partial y}{\partial \mathbf{x}} = \frac{\partial \mathbf{x^TAx}}{\partial \mathbf{x}} = \frac{\partial \sum a_{ij}x_ix_j}{\partial \mathbf{x}} = \left[\sum a_{1j}x_j + \sum a_{i1}x_i, \sum a_{2j}x_j + \sum a_{i2}x_i, \cdots, \sum a_{nj}x_j + \sum a_{in}x_i\right] = \mathbf{x}^T(\mathbf{A} + \mathbf{A}^T)
$$&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Matrix_calculus#Vector-by-vector_identities&#34;&gt;Wikipedia: Matrix Calculus&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</description>
                
                
                
                
                
                    
                        
                            
                                
                                
                                
                                    <category domain="https://zchencn.github.io/categories/math/">Math</category>
                                
                            
                        
                    
                        
                            
                                
                                
                                
                                    <category domain="https://zchencn.github.io/tags/matrix-caculus/">Matrix Caculus</category>
                                
                            
                        
                    
                
            </item>
        
            <item>
                <title>Static Library and Dynamic Link Library</title>
                <link>https://zchencn.github.io/post/staticlibanddynamiclib/</link>
                <guid isPermaLink="true">https://zchencn.github.io/post/staticlibanddynamiclib/</guid>
                <pubDate>Thu, 03 Dec 2020 00:00:00 &#43;0000</pubDate>
                
                    <author>zchen@lsec.ac.cc.cn (zchen)</author>
                
                <copyright>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</copyright>
                
                    <description>&lt;h2 id=&#34;静态库和动态库&#34;&gt;静态库和动态库&lt;/h2&gt;
&lt;p&gt;将频繁复用的程序打包成库文件，并在使用时链接到项目中可以有效的节省编译时间，避免频繁的复制，提高编程的效率。 常用有静态库(static library)和动态库(shared library or dynamic library). &lt;br&gt;
程序的编译运行可分为下面四个步骤：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;预编译；处理预编译指导语句，也就是程序中以#开头的行，如#include和#define等；&lt;/li&gt;
&lt;li&gt;编译；将程序的源文件(.c)转化为目标文件(.o);&lt;/li&gt;
&lt;li&gt;链接；将库和所有目标文件链接成可执行程序。对静态库来说，实际的文件被打包进了最终的可执行程序，然而动态库只是把标记放入的可执行程序；&lt;/li&gt;
&lt;li&gt;装载；程序被装载程序(loader)装载后开始执行，所有动态库的标记都会被解析被映射到程序中；&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;动态库&#34;&gt;动态库&lt;/h3&gt;
&lt;p&gt;编译和链接使用动态库有如下步骤：&lt;/p&gt;
&lt;h4 id=&#34;step-1-compiling-with-position-independent-code&#34;&gt;Step 1: Compiling with Position Independent Code&lt;/h4&gt;
&lt;p&gt;我们需要将库的源文件编译成位置独立的代码(PIC):&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;gcc -c -Wall -Werror -fpic source_file.c
&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h4 id=&#34;step-2-creating-a-shared-library-from-an-object-file&#34;&gt;Step 2: Creating a shared library from an object file&lt;/h4&gt;
&lt;p&gt;将目标文件打包成动态库：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;gcc -shared -o libshared_lib_name.so source_file.o
&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h4 id=&#34;step-3-linking-with-a-shared-library&#34;&gt;Step 3: Linking with a shared library&lt;/h4&gt;
&lt;p&gt;经过前两步我们已经有了一个动态库了。现在main.c文件需要使用动态库，我们使用-L选项告诉编译器库所在的目录，并用-l选型指定库的名字：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;gcc -Wall -L directory_of_library -o &lt;span class=&#34;nb&#34;&gt;test&lt;/span&gt; main.c -lshared_lib_name
&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h4 id=&#34;step-4-making-the-library-available-at-runtime&#34;&gt;Step 4: Making the library available at runtime&lt;/h4&gt;
&lt;p&gt;然而此时执行程序还是会报错：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;error while loading shared libraries: shared_lib_name.so: cannot open shared object file: No such file or directory
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;有两种方法解决这个问题，一种是将库目录加到LD_LIBRARY_PATH变量中：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;nb&#34;&gt;export&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;LD_LIBRARY_PATH&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;directory_of_library:&lt;span class=&#34;nv&#34;&gt;$LD_LIBRARY_PATH&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;一种是在编译链接时加上：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;-Wl,-rpath=directory_of_library
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;-Wl是将后面的参数传给链接器的意思，“,”用来处理参数中的空格。&lt;/p&gt;
</description>
                
                
                
                
                
                    
                        
                            
                                
                                
                                
                                    <category domain="https://zchencn.github.io/categories/computer-science/">Computer Science</category>
                                
                            
                                
                                
                                
                                    <category domain="https://zchencn.github.io/categories/programming-language/">Programming Language</category>
                                
                            
                                
                                
                                
                                    <category domain="https://zchencn.github.io/categories/cc-/">C&amp;C&#43;&#43;</category>
                                
                            
                        
                    
                        
                            
                                
                                
                                
                                    <category domain="https://zchencn.github.io/tags/static-library/">Static Library</category>
                                
                            
                                
                                
                                
                                    <category domain="https://zchencn.github.io/tags/dynamic-link-library/">Dynamic Link Library</category>
                                
                            
                        
                    
                
            </item>
        
            <item>
                <title>SEM</title>
                <link>https://zchencn.github.io/post/sem/</link>
                <guid isPermaLink="true">https://zchencn.github.io/post/sem/</guid>
                <pubDate>Fri, 23 Oct 2020 00:00:00 &#43;0000</pubDate>
                
                    <author>zchen@lsec.ac.cc.cn (zchen)</author>
                
                <copyright>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</copyright>
                
                    <description>&lt;h1 id=&#34;谱元法简介&#34;&gt;谱元法简介&lt;/h1&gt;
&lt;p&gt;谱元法(&lt;strong&gt;S&lt;/strong&gt;pectral &lt;strong&gt;E&lt;/strong&gt;lement &lt;strong&gt;M&lt;/strong&gt;ethod，简称SEM)结合了谱方法的高精度性和有限元方法对于不规则求解区域的灵活性。&lt;br&gt;
谱元法的求解一般可以分为五个步骤：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;将PDE改写成弱形式(Weak Form)&lt;/li&gt;
&lt;li&gt;将求解区域分隔为若干个小的单元&lt;/li&gt;
&lt;li&gt;用有限空间内的分片多项式函数近未知函数&lt;/li&gt;
&lt;li&gt;代入弱形式并数值求解上述过程得到的积分得到单元上的矩阵表示&lt;/li&gt;
&lt;li&gt;将矩阵拼装得到最后求解的线性方程组&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下面用一个例子具体说明这些步骤。&lt;br&gt;
考虑如下赫姆霍兹方程(Helmholtz Equation)：
$$
-\nabla^2u + ku = f, x\in \Omega = [a, b]
$$
以及如下迪利克雷边界条件(Dirchlet Boundary Condition)：
$$
u(a) = u(b) = 0
$$&lt;/p&gt;
&lt;h2 id=&#34;弱形式&#34;&gt;弱形式&lt;/h2&gt;
&lt;p&gt;设好的函数$v$同样满足迪利克雷边界条件，我们称它为测试函数(Test Function). 在方程两端同时乘上$v$并且在整个求解区域上做积分，运用分部积分公式可得：
$$
\int_a^b\frac{\partial u}{\partial x}\frac{\partial v}{\partial x} \ dx + k\int_a^buv \ dx = \int_a^b fv \ dx
$$
可以证明，对任意测试函数都满足上式的$u$与原方程的解等价。&lt;br&gt;
严谨的证明参考有限元方法数学理论的相关书籍。&lt;/p&gt;
&lt;h2 id=&#34;剖分&#34;&gt;剖分&lt;/h2&gt;
&lt;p&gt;将$\Omega = [a, b]$剖分成$N_e$个小单元
$$\Omega = \cup_{e=1}^{N_e}\Omega_e, \Omega_e = [X_{e-1}, X_e], X_0 = a, X_{N_e} = b$$
为方便后续的讨论和求积分，建立一个标准单元（参考单元）到$\Omega_e$的映射：
$$
F^e:  [-1, 1] \longrightarrow \Omega_e, x = F^e(\xi) = \frac{X_{e} - X_{e-1}}{2}\xi + \frac{X_e + X_{e-1}}{2}
$$
相反的有：
$$
\xi = (F^e)^{-1}(x) = \frac{2x - X_e - X_{e-1}}{X_e - X_{e-1}}
$$
二维的情况类似, 其中$\Omega_e$变成顶点为$P^e_1, P^e_2, P^e_3, P^e_4$(从左下按逆时针顺序编号)的四边形，同样的可以建立标准单元到$\Omega_e$的映射：
$$
F^e:  [-1, 1]^2 \longrightarrow \Omega_e, F^e(\xi, \eta) = \sum_{a=1}^4P^e_aN_a(\xi, \eta)
$$
其中：
$$
N_1 = \frac{1-\xi}{2}\cdot\frac{1-\eta}{2}, \ N_2 = \frac{1 + \xi}{2}\cdot\frac{1 - \eta}{2} \ N_3 = \frac{1 + \xi}{2}\cdot\frac{1 + \eta}{2}, \ N_4 = \frac{1 - \xi}{2}\cdot \frac{1 + \eta}{2}
$$&lt;/p&gt;
&lt;h2 id=&#34;多项式近似&#34;&gt;多项式近似&lt;/h2&gt;
&lt;p&gt;在每个单元上我们使用GLL(Gauss-Legendre-Labotto)求积公式节点的拉格朗日(Lagrange)多项式来近似函数。假设GLL点为$\xi_0 = -1, \xi_1, \cdots, \xi_n = 1$，则各个单元上的节点为：
$$
x^e_i = F^e(\xi_i), i=0,1, \cdots,n
$$
记全局节点为：
$$
x_I = x^e_i , I = I(e, i) = (e - 1)n + i
$$
$I$可以取从$0$到$N_e\times n$的所有整数，共$N_e\times n + 1$个。
在每个单元上，我们有：
$$
u(x) = u(x(\xi)) = u(F^e(\xi)) = u^e(\xi) \approx \sum_{i=0}^nu^e_il_i(\xi) = \sum_{i=0}^nu^e_il_i((F^e)^{-1}(x)) = \sum_{i=0}^nu^e_i\phi^e_i
$$
其中：
$$
u^e_i = u^e(\xi_i) = u(F^e(\xi_i)), \phi_i^e(x) = l_i((F^e)^{-1}(x))
$$
对于一阶导数，我们有:&lt;/p&gt;
&lt;!--
$$
\frac{\partial u}{\partial x} = \frac{\partial u^e(\xi)}{\partial \xi}\frac{\partial \xi}{\partial x} = \frac{2}{L_e}\frac{\partial u^e(\xi)}{\partial \xi} \approx \frac{2}{L_e}\sum_{i=0}^nu^e_i\frac{\partial l_i}{\partial \xi}, \ L_e = X_e - X_{e-1} 
$$
--&gt;
&lt;p&gt;$$
\frac{\partial u}{\partial x} \approx \sum_{i=0}^n
u^e_i\frac{\partial \phi^e_i}{\partial x}$$&lt;/p&gt;
&lt;p&gt;对于二维的情况，我们选择的插值基函数是拉格朗日多项式的乘积，记：
$$
\phi_I(\xi, \eta) = l_i(\xi)l_j(\eta), I = i + j(n + 1)
$$
则在每个子区域上：
$$
u(x, y) = u(F^e(\xi, \eta)) = u^e(\xi, \eta) \approx \sum_{i,j=0}^{1+n}u^e_{i,j}l_i(\xi)l_j(\eta) = \sum_{I=0}^{(n+1)^2}u^e_I\phi_I(\xi, \eta) = \sum_{I=0}^{(n+1)^2}u^e_I\phi_I^e(x, y)
$$
其中：
$$
u^e_{i,j} = u^e(\xi_i, \eta_j) = u(F^e(\xi_i, \eta_j)), u^e_I = u^e_{i, j} \
\phi_I^e(x, y) = \phi_I((F^e)^{-1}(x, y)), I = i + j(n + 1),\ i,j = 0, 1, \cdots, n
$$
对于偏导数：&lt;/p&gt;
&lt;!--
$$
\frac{\partial u}{\partial x} = \frac{\partial u^e}{\partial \xi}\frac{\partial \xi}{\partial x} + \frac{\partial u^e}{\partial \eta}\frac{\partial \eta}{\partial x} \approx \sum_{i,j = 0}^{n+1}u^e_{i,j}\left(\frac{\partial l_i(\xi)}{\partial \xi}l_j(\eta)\frac{\partial \xi}{\partial x} + \frac{\partial l_j(\eta)}{\partial \eta}l_i(\xi)\frac{\partial \eta}{\partial x}\right) \\

\frac{\partial u}{\partial y} = \frac{\partial u^e}{\partial \xi}\frac{\partial \xi}{\partial y} + \frac{\partial u^e}{\partial \eta}\frac{\partial \eta}{\partial y} \approx \sum_{i,j = 0}^{n+1}u^e_{i,j}\left(\frac{\partial l_i(\xi)}{\partial \xi}l_j(\eta)\frac{\partial \xi}{\partial y} + \frac{\partial l_j(\eta)}{\partial \eta}l_i(\xi)\frac{\partial \eta}{\partial y}\right)
$$
--&gt;
&lt;p&gt;$$
\frac{\partial u}{\partial x} \approx \sum_{I=0}^{(n+1)^2}u^e_I\frac{\partial \phi^e_I}{\partial x}, \frac{\partial u}{\partial y} \approx \sum_{I=0}^{(1+n)^2}u^e_I\frac{\partial \phi^e_I}{\partial y}
$$&lt;/p&gt;
&lt;h2 id=&#34;数值积分&#34;&gt;数值积分&lt;/h2&gt;
&lt;p&gt;在每个单元上, 选取测试函数为任意插值基函数，由方程的弱形式可以得到：
$$
\int_{\Omega_e} u\phi^e_j\ dx \approx \sum_{i=0}^nu^e_i\int_{\Omega_e} \phi^e_i\phi^e_j\ dx = \sum_{i=0}^nM^e_{j ,i}u^e_i, \forall j = 0, 1, \cdots, n
$$
其中：
$$
M^e_{j,i} = \int_{\Omega_e}\phi^e_i\phi^e_j\ dx = \int_{-1}^1l_i(\xi)l_j(\xi)\frac{\partial x}{\partial \xi}\ d\xi \approx \frac{L_e}{2}w_i\delta_{j, i}
$$
所以单元质量矩阵(Mass Matrix)是一个对角矩阵。&lt;br&gt;
再考虑含有导数的部分：
$$
\int_{\Omega_e}\frac{\partial u}{\partial x}\frac{\partial \phi^e_j}{\partial x}\ dx \approx \sum_{i=0}^nu^e_i\int_{\Omega_e}\frac{\partial \phi^e_i}{\partial x}\frac{\partial \phi^e_j}{\partial x}\ dx = \sum_{i=0}^nK^e_{j, i}u^e_i, \forall j = 0, 1, \cdots, n
$$
其中
$$
K^e_{j, i} = \int_{\Omega_e}\frac{\partial \phi^e_i}{\partial x}\frac{\partial \phi^e_j}{\partial x}\ dx = \int_{-1}^1\left(\frac{\partial \xi}{\partial x}\right)^2\frac{\partial l_i}{\partial \xi}\frac{\partial l_j}{\partial \xi}\frac{\partial x}{\partial \xi}\ d\xi = \frac{2}{L_e}\sum_{k=0}^nw_kd_{i,k}d_{j, k}
$$
$$
d_{i, k} = l^{&#39;}_i(\xi_k), d_{j, k} = l_j^{&#39;}(\xi_k)
$$
我们有：
$$
K^e = \left(\frac{2}{L_e}\right)^2DM^eD^t
$$
单元刚度矩阵(Stifness Matrix)$K^e$是对称矩阵。
最后，右端项：
$$
f^e_j = \int_{\Omega_e}f\phi^e_j\ dx = \frac{L_e}{2}\int_{-1}^1f(F^e(\xi))l_j(\xi)\ d\xi \approx \frac{w_jL_e}{2}f(F^e(\xi_j))
$$
对于二维的情况：
$$
\int_{\Omega_e}u\phi^e_J\ dS \approx \sum_{I=0}^{(1+n)^2}u^e_I\int_{\Omega_e}\phi^e_I\phi^e_J\ dS = \sum_{I=0}^{(1+n)^2}M^e_{J, I}u^e_I
$$
其中：
$$
M^e_{J, I} = \int_{\Omega_e}\phi^e_I\phi^e_J\ dS = \int_{-1}^1\int_{-1}^1\phi_I\phi_J J^e\ d\xi d\eta = w_iw_jJ^e(\xi_i, \xi_j)\delta_{I, J}, I = I(i, j) = J
$$&lt;/p&gt;
&lt;h2 id=&#34;线性方程组&#34;&gt;线性方程组&lt;/h2&gt;
&lt;p&gt;弱形式经过上面的近似和数值积分，得到：
$$
\sum_{e=1}^{N_e}\sum_{i=0}^nK^e_{j,i}u^e_i + k\sum_{e=1}^{N_e}\sum_{i=0}^{n}M^e_{j,i}u^e_i = \sum_{e=1}^{N_e}f^e_j
$$
全局质量矩阵：
$$
M = \sum_{e=1}^{N_e}M^e =
\begin{pmatrix}
M^1_{0,0} &amp;amp;        &amp;amp;                     &amp;amp; \\
&amp;amp; \ddots &amp;amp;                     &amp;amp; \\
&amp;amp;        &amp;amp; M^1_{n,n}+M^2_{0,0} &amp;amp; \\
&amp;amp;        &amp;amp;                     &amp;amp; \ddots
\end{pmatrix}
$$
$M$是一个$n\times N_e + 1$的方阵。&lt;br&gt;
全局刚度矩阵:
$$
K = \sum_{e=1}^{N_e}K^e = \begin{pmatrix}
K^1_{0,0} &amp;amp; \cdots &amp;amp; K^1_{0, n}                     &amp;amp; \\
\vdots        &amp;amp; \ddots &amp;amp;         \vdots            &amp;amp; \\
K^1_{n, 0}        &amp;amp; \cdots       &amp;amp; M^1_{n,n}+M^2_{0,0} &amp;amp; \\
&amp;amp;        &amp;amp;                     &amp;amp; \ddots
\end{pmatrix}
$$
同样也是一个$n\times N_e + 1$的方阵。&lt;br&gt;
则最终的线性方程组是：
$$
(K +kM)\underline{u} = \underline{f}
$$
其中：
$$
\underline{u} = (u_0, u_1, \cdots, u_{n\times N_e + 1})^t \\
\underline{f} = (f^1_0, \cdots,f^1_n + f^2_0, \cdots, f^{N_e}_n)^t
$$&lt;/p&gt;
&lt;p&gt;二维的情况类似，这里就不再赘述。&lt;/p&gt;
&lt;p&gt;求解上述得到的线性方程组，就可以得到微分方程的数值解。&lt;/p&gt;
</description>
                
                
                
                
                
                    
                        
                            
                                
                                
                                
                                    <category domain="https://zchencn.github.io/categories/math/">Math</category>
                                
                            
                                
                                
                                
                                    <category domain="https://zchencn.github.io/categories/numerical-analysis/">Numerical Analysis</category>
                                
                            
                                
                                
                                
                                    <category domain="https://zchencn.github.io/categories/numerical-quadrature/">Numerical Quadrature</category>
                                
                            
                        
                    
                        
                            
                                
                                
                                
                                    <category domain="https://zchencn.github.io/tags/sem/">SEM</category>
                                
                            
                                
                                
                                
                                    <category domain="https://zchencn.github.io/tags/pde/">PDE</category>
                                
                            
                        
                    
                
            </item>
        
            <item>
                <title>Gaussian Quadrature</title>
                <link>https://zchencn.github.io/post/gaussianquadrature/</link>
                <guid isPermaLink="true">https://zchencn.github.io/post/gaussianquadrature/</guid>
                <pubDate>Sat, 27 Jun 2020 00:00:00 &#43;0000</pubDate>
                
                    <author>zchen@lsec.ac.cc.cn (zchen)</author>
                
                <copyright>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</copyright>
                
                    <description>&lt;h1 id=&#34;高斯求积&#34;&gt;高斯求积&lt;/h1&gt;
&lt;h2 id=&#34;数值积分&#34;&gt;数值积分&lt;/h2&gt;
&lt;p&gt;考虑带权的积分如下：
$$
\int_a^bf(x)w(x)dx
$$
其中 $w(x) \geq 0, \int_a^bw(x)dx &amp;gt; 0$，称$w(x)$为权。一般的数值积分公式有如下的形式：
$$
\int_a^bw(x)f(x)dx \approx \sum_{i=0}^nw_if(x_i)
$$
即用$n+1$ 个函数值的加权和来近似积分的值。&lt;/p&gt;
&lt;p&gt;我们记以$x_i(i=0,1,\cdots,n)$ 为节点的拉格朗日(Langrange)插值多项式为：
$$
L_n(x)=\sum_{i=1}^nf(x_i)l_i(x)
$$
其中$l_i(x)$是拉格朗日插值基函数，即：
$$
l_i(x) = \frac{\prod_{k=0,k\neq i}^n(x-x_k)}{\prod_{k=0,k\neq i}^n(x_i-x_k)}
$$
则：
$$
f(x)=L_n(x)+R[f],\quad R[f]=\frac{1}{(n+1)!}f^{(n+1)}(\xi)(x-x_0)(x-x_1)\cdots(x-x_n)
$$&lt;/p&gt;
&lt;p&gt;有：
$$
\int_a^bw(x)f(x)dx=\left(\sum_{i=0}^n\int_a^bw(x)l_i(x)dx\right)f(x_i)+\int_a^bw(x)R[f]dx
$$&lt;/p&gt;
&lt;p&gt;一般我们取$w_i =\int_a^bw(x)l_i(x)dx$ ，则数值积分公式的误差就是上式等号右侧的第二项，当$f(x)$ 是不超过$n$ 次的多项式时，容易看出误差为0。若数值积分公式对不超过$k$ 次的多项式精确成立，我们就称它的&lt;strong&gt;代数精度&lt;/strong&gt;为$k$ 。所以上述数值积分公式的代数精度至少为$n$。&lt;/p&gt;
&lt;p&gt;另一方面，数值积分公式中含有$n+1$ 个$w_i$ 和$n+1$个$x_i$ ，共$2n+2$ 个自由度，所以可以想象通过适当选取节点$x_i$ ，它的代数精度最多可以为$2n+1$ 。我们把具有$2n+1$ 次代数精度的求积公式称为&lt;strong&gt;高斯求积公(GaussianQuadrature)&lt;/strong&gt;，其节点$x_i(i=0,1,\cdots,n)$ 称为&lt;strong&gt;高斯点&lt;/strong&gt;。接下来讨论怎么选取合适的点作为高斯点。&lt;/p&gt;
&lt;h2 id=&#34;正交多项式与高斯点&#34;&gt;正交多项式与高斯点&lt;/h2&gt;
&lt;p&gt;称多项式$p(x), q(x)$ （带权）正交如果：
$$
\int_a^bw(x)p(x)q(x)dx=0
$$
假设以$x_i(i=0,1,\cdots,n)$ 为零点的多项式$p(x)=(x-x_0)(x-x_1)\cdots(x-x_n)$ 与任何不超过$n$次的多项式正交，由多项式的带余除法可知，对于不超过$2n+1$次的多项式$f(x)$ ，有不超过$n$ 次的多项式$q(x), r(x)$ 使得：
$$
f(x)=p(x)q(x)+r(x)
$$
那么：
$$
\int_a^bw(x)f(x)dx=\int_a^bw(x)p(x)q(x)dx+\int_a^bw(x)r(x)dx=\int_a^bw(x)r(x)dx=\sum_{i=0}^nw_ir(x_i)
$$
又：
$$
f(x_i)=p(x_i)q(x_i)+r(x_i)=r(x_i)
$$
所以：
$$
\int_a^bw(x)f(x)dx=\sum_{i=0}^nw_if(x_i)
$$
通过这种方式，我们发现只要选取节点为正交多项式的零点就可以得到高斯求积公式。&lt;/p&gt;
&lt;h2 id=&#34;gauss-legendre&#34;&gt;Gauss-Legendre&lt;/h2&gt;
&lt;p&gt;取$[a,b]=[-1,1],w(x)=1$，由${1,x,x^2,\cdots}$ 正交化得到的多项式称为勒朗德(Legendre)多项式，一般记为$L_n(x)$ 。我们只要选取节点为$L_{n+1}(x)$ 的零点就可以得到高斯-勒朗德求积公式。&lt;/p&gt;
&lt;h2 id=&#34;gauss-chebyshev&#34;&gt;Gauss-Chebyshev&lt;/h2&gt;
&lt;p&gt;取$[a,b]=[-1,1],w(x)=\frac{1}{\sqrt{1-x^2}}$，由${1,x,x^2,\cdots}$ 正交化得到的多项式称为切比雪夫(chebyshev)多项式，一般记为$T_n(x)$ 。我们只要选取节点为$T_{n+1}(x)$ 的零点就可以得到高斯-切比雪夫求积公式。&lt;/p&gt;
&lt;!--

## Gauss-Radau

取$[a,b]=[-1,1]$ ，且固定$x_0=-1$ ，取：
$$
p(x)=p_{n+1}(x)+ap_n(x)
$$
取合适的$a$ 使得$p(-1)=0$ ，$x_1,x_2,\cdots,x_n$ 是$p(x)$剩余的零点，则对任意次数不超过$2n$ 的多项式$f(x)$ ：
$$
f(x)=p(x)q(x)+r(x),
$$
$r(x)$ 的次数不超过$n$ ，$q(x)$ 的次数不超过$n-1$ ，且$f(x_i)=r(x_i)$ ，那么
$$
\sum_{i=0}^nw_if(x_i)=\sum_{i=0}^nw_ir(x_i)=\int_{-1}^1w(x)r(x)dx=\int_{-1}^1w(x)f(x)-w(x)p(x)q(x)dx
$$
其中：
$$
\int_{-1}^1w(x)p(x)q(x)dx=\int_{-1}^1w(x)p_{n+1}(x)q(x)dx+a\int_{-1}^1w(x)p_n(x)q(x)dx=0
$$
于是有：
$$
\int_{-1}^1w(x)f(x)dx=\sum_{i=1}^nw_if(x_i),\quad \forall f(x)\in P_{2n}
$$
--&gt;
&lt;h2 id=&#34;gauss-labotto&#34;&gt;Gauss-Labotto&lt;/h2&gt;
&lt;p&gt;取$[a,b]=[-1,1]$ ，如果我们想在求积节点中包含两个区间端点，即固定$x_0=-1,x_n=1$，这种方式称为Gauss-Labotto求积。此时，自由度只有$2n$($n+1$个权和$n-1$节点)，可以想象至多达到$2n-1$阶代数精度。&lt;br&gt;
对任意2n-1次多项式$p_{2n-1}$，同样由多项式的带余除法可以得到：
$$
p_{2n-1} - L_n= (1-x^2)(x-x_1)\cdots(x-x_{n-1})q(x) + r(x),
$$
其中$q(x)\in P_{n-2}, r(x)\in P_n$，且$r(x_i) = 0(i = 0, 1, \cdots,n)$，所以$r(x) \equiv 0$，那么数值积分的误差项为：
$$
E(x) = \int_{-1}^1(1-x^2)(x-x_1)\cdots(x-x_{n})q(x)
$$
我们选取$x_1, x_2, \cdots, x_{n-1}$是$n-1$次Labotto多项式的零点，有$E(x)=0$.&lt;/p&gt;
</description>
                
                
                
                
                
                    
                        
                            
                                
                                
                                
                                    <category domain="https://zchencn.github.io/categories/math/">Math</category>
                                
                            
                                
                                
                                
                                    <category domain="https://zchencn.github.io/categories/numerical-analysis/">Numerical Analysis</category>
                                
                            
                                
                                
                                
                                    <category domain="https://zchencn.github.io/categories/numerical-quadrature/">Numerical Quadrature</category>
                                
                            
                        
                    
                        
                            
                                
                                
                                
                                    <category domain="https://zchencn.github.io/tags/numerical-quadrature/">Numerical Quadrature</category>
                                
                            
                        
                    
                
            </item>
        
            <item>
                <title>An Introduction to Make</title>
                <link>https://zchencn.github.io/post/makefiles/</link>
                <guid isPermaLink="true">https://zchencn.github.io/post/makefiles/</guid>
                <pubDate>Sat, 21 Dec 2019 00:00:00 &#43;0000</pubDate>
                
                    <author>zchen@lsec.ac.cc.cn (zchen)</author>
                
                <copyright>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</copyright>
                
                    <description>&lt;h2 id=&#34;概述&#34;&gt;概述&lt;/h2&gt;
&lt;p&gt;make是控制如何从源文件(source file)生成可执行文件(excutable)及其他非源文件(non-source file)的一种工具。make工具通过makefile中说明的方式，构建(build)整个程序(program)。在一个庞大的项目中可能包含很多个源文件，修改少部分源文件可能需要重新编译整个程序，这个过程耗时耗力，通过使用make工具可以只重新编译依赖于修改过的文件的部分，从而提升效率；&lt;/p&gt;
&lt;h2 id=&#34;makefile&#34;&gt;makefile&lt;/h2&gt;
&lt;h3 id=&#34;makefile的基本语法&#34;&gt;makefile的基本语法&lt;/h3&gt;
&lt;p&gt;makefile主要由一条条如下的规则组成：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-makefile&#34; data-lang=&#34;makefile&#34;&gt;&lt;span class=&#34;nf&#34;&gt;target ... &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;prerequisite&lt;/span&gt; ...
	&lt;span class=&#34;nb&#34;&gt;command&lt;/span&gt;
	...
	...
&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;注意其中命令行前面是&lt;strong&gt;Tab&lt;/strong&gt;。
如果一条规则的目标属于以下情况之一，就称为需要更新：&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;目标没有生成；&lt;/li&gt;
&lt;li&gt;某个条件需要更新；&lt;/li&gt;
&lt;li&gt;某个条件的修改时间比目标晚；&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;在一条规则被执行前，规则的条件可能处于以下三种状态之一：&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;需要更新。能够找到以该条件为目标的规则，且该规则中目标需要更新；&lt;/li&gt;
&lt;li&gt;不需要更新。能够找到以该条件为目标的规则，但是该规则中目标不需要更新；或者不能找到以该条件为目标的规则且该条件已经生成；&lt;/li&gt;
&lt;li&gt;错误。不能找到以该条件为目标的规则，并且该条件没有生成；&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;执行一条规则A的步骤如下&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;检查它的每个条件P：&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;如果P需要更新，则执行以P为目标的规则B。之后无论是否生成文件P都认为P已经被更新；&lt;/li&gt;
&lt;li&gt;如果找不到规则B，并且文件P已经存在，则表示P不需要更新；&lt;/li&gt;
&lt;li&gt;如果找不到规则B，并且文件P不存在，则报错退出；&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;检查完A的所有条件后，检查它的目标T，如果属于一下情况之一，就执行命令列表：&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;文件T不存在；&lt;/li&gt;
&lt;li&gt;文件T存在，但是某个条件的修改时间比它晚；&lt;/li&gt;
&lt;li&gt;某个条件P已将被更新（并不一定生成文件P，只要执行了命令列表就视为已经更新）；&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;在shell中通过以下命令执行&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;$ make &lt;span class=&#34;c1&#34;&gt;# 从缺省目标开始更新，即makefile中第一条规则的目标&lt;/span&gt;
$ make target &lt;span class=&#34;c1&#34;&gt;# 更新target这个目标&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h3 id=&#34;变量&#34;&gt;变量&lt;/h3&gt;
&lt;p&gt;makefile可以用‘=’定义变量和&#39;$&amp;lsquo;读取变量的值：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-makefile&#34; data-lang=&#34;makefile&#34;&gt;&lt;span class=&#34;err&#34;&gt;$&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;CC&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; gcc &lt;span class=&#34;c1&#34;&gt;# 定义变量CC，值是gcc&lt;/span&gt;
&lt;span class=&#34;err&#34;&gt;$&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;$(&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;CC&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;c&#34;&gt;# 取出CC的值
&lt;/span&gt;&lt;span class=&#34;c&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;$ CC &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;:&lt;/span&gt;= &lt;span class=&#34;n&#34;&gt;gcc&lt;/span&gt; &lt;span class=&#34;c&#34;&gt;# 读到$(CC)时立刻展开
&lt;/span&gt;&lt;span class=&#34;c&#34;&gt;&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;$&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;CC&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;?=&lt;/span&gt; gcc &lt;span class=&#34;c1&#34;&gt;# CC没有定义过则等同&amp;#39;=&amp;#39;，否则什么也不做&lt;/span&gt;
&lt;span class=&#34;err&#34;&gt;$&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;CC&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; gcc &lt;span class=&#34;c1&#34;&gt;# CC可以追加定义&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;makefile中还有一些特殊变量：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-makefile&#34; data-lang=&#34;makefile&#34;&gt;&lt;span class=&#34;err&#34;&gt;$&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;$@&lt;/span&gt; &lt;span class=&#34;c&#34;&gt;# 表示规则中的目标
&lt;/span&gt;&lt;span class=&#34;c&#34;&gt;&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;$&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;$&amp;lt;&lt;/span&gt; &lt;span class=&#34;c&#34;&gt;# 表示规则中的第一个条件
&lt;/span&gt;&lt;span class=&#34;c&#34;&gt;&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;$&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;$?&lt;/span&gt; &lt;span class=&#34;c&#34;&gt;# 表示规则中所有比目标新的条件，组成一个列表，以空格分隔
&lt;/span&gt;&lt;span class=&#34;c&#34;&gt;&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;$&lt;/span&gt; &lt;span class=&#34;err&#34;&gt;$^&lt;/span&gt; &lt;span class=&#34;c&#34;&gt;# 表示规则中所有的条件，组成一个列表，以空格分隔
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;makefile中的隐含变量，有的变量已经定义了缺省值：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-makefile&#34; data-lang=&#34;makefile&#34;&gt;&lt;span class=&#34;err&#34;&gt;AR&lt;/span&gt; &lt;span class=&#34;err&#34;&gt;静态库打包命令，缺省值ar&lt;/span&gt;
&lt;span class=&#34;err&#34;&gt;ARFLAGS&lt;/span&gt; &lt;span class=&#34;err&#34;&gt;静态库打包命令的选型，缺省值rv&lt;/span&gt;
&lt;span class=&#34;err&#34;&gt;AS&lt;/span&gt; &lt;span class=&#34;err&#34;&gt;汇编器的名字，缺省值as&lt;/span&gt;
&lt;span class=&#34;err&#34;&gt;ASFLAGS&lt;/span&gt; &lt;span class=&#34;err&#34;&gt;汇编器的选项，没有定义&lt;/span&gt;
&lt;span class=&#34;err&#34;&gt;CC&lt;/span&gt; &lt;span class=&#34;err&#34;&gt;C编译器的名字，缺省值是cc&lt;/span&gt;
&lt;span class=&#34;err&#34;&gt;CFLAGS&lt;/span&gt; &lt;span class=&#34;err&#34;&gt;C编译器的选项，没有定义&lt;/span&gt;
&lt;span class=&#34;err&#34;&gt;CXX&lt;/span&gt; &lt;span class=&#34;err&#34;&gt;C++编译器的名字，缺省值是g++&lt;/span&gt;
&lt;span class=&#34;err&#34;&gt;CXXFLAGS&lt;/span&gt; &lt;span class=&#34;err&#34;&gt;C++编译器的选项，没有定义&lt;/span&gt;
&lt;span class=&#34;err&#34;&gt;CPP&lt;/span&gt; &lt;span class=&#34;err&#34;&gt;C预处理器的名字，缺省值是&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;$(&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;CC&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;err&#34;&gt;-E&lt;/span&gt;
&lt;span class=&#34;err&#34;&gt;CPPFLAGS&lt;/span&gt; &lt;span class=&#34;err&#34;&gt;C预处理器的选项，没有定义&lt;/span&gt;
&lt;span class=&#34;err&#34;&gt;LD&lt;/span&gt; &lt;span class=&#34;err&#34;&gt;链接器的名字，缺省值是ld&lt;/span&gt;
&lt;span class=&#34;err&#34;&gt;OUTPUT_OPTION&lt;/span&gt; &lt;span class=&#34;err&#34;&gt;输出的命令行选项，缺省值是-o&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;$@&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;。&lt;/span&gt;
&lt;span class=&#34;err&#34;&gt;RM&lt;/span&gt; &lt;span class=&#34;err&#34;&gt;删除命令的名字，缺省值是rm&lt;/span&gt; &lt;span class=&#34;err&#34;&gt;-f&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h3 id=&#34;常用make命令选项&#34;&gt;常用make命令选项&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-makefile&#34; data-lang=&#34;makefile&#34;&gt;&lt;span class=&#34;err&#34;&gt;$&lt;/span&gt; &lt;span class=&#34;err&#34;&gt;make&lt;/span&gt; &lt;span class=&#34;err&#34;&gt;-n&lt;/span&gt; 
&lt;span class=&#34;c&#34;&gt;# 打印要执行的命令而不执行
&lt;/span&gt;&lt;span class=&#34;c&#34;&gt;# 这个命令可以用于查看命令的执行顺序，确认无误了再执行命令
&lt;/span&gt;&lt;span class=&#34;c&#34;&gt;&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;$&lt;/span&gt; &lt;span class=&#34;err&#34;&gt;make&lt;/span&gt; &lt;span class=&#34;err&#34;&gt;-C&lt;/span&gt; &lt;span class=&#34;c&#34;&gt;# 可以切换到另一个目录执行makefile
&lt;/span&gt;&lt;span class=&#34;c&#34;&gt;&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h3 id=&#34;例子&#34;&gt;例子&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;16
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;17
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;18
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;19
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;20
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;21
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;22
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;23
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;24
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;25
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;26
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;27
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;28
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;29
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;30
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;31
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;32
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;33
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;34
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;35
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;36
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;37
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;38
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;39
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;40
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;41
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;42
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;43
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;44
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;45
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;46
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;47
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;48
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;49
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;50
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;51
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;52
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;53
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;54
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;55
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;56
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;57
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;58
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;59
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;60
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;61
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;62
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;63
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;64
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;65
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;66
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;67
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;68
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;69
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;70
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;71
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;72
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;73
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;74
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;75
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;76
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-makefile&#34; data-lang=&#34;makefile&#34;&gt;&lt;span class=&#34;c&#34;&gt;# A sample Makefile
&lt;/span&gt;&lt;span class=&#34;c&#34;&gt;# This Makefile demonstrates and explains 
&lt;/span&gt;&lt;span class=&#34;c&#34;&gt;# Make Macros, Macro Expansions,
&lt;/span&gt;&lt;span class=&#34;c&#34;&gt;# Rules, Targets, Dependencies, Commands, Goals
&lt;/span&gt;&lt;span class=&#34;c&#34;&gt;# Artificial Targets, Pattern Rule, Dependency Rule.
&lt;/span&gt;&lt;span class=&#34;c&#34;&gt;&lt;/span&gt;
&lt;span class=&#34;c&#34;&gt;# Comments start with a # and go to the end of the line.
&lt;/span&gt;&lt;span class=&#34;c&#34;&gt;&lt;/span&gt;
&lt;span class=&#34;c&#34;&gt;# Here is a simple Make Macro.
&lt;/span&gt;&lt;span class=&#34;c&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;LINK_TARGET&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; test_me.exe

&lt;span class=&#34;c&#34;&gt;# Here is a Make Macro that uses the backslash to extend to multiple lines.
&lt;/span&gt;&lt;span class=&#34;c&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;OBJS&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;  &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt; Test1.o &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt; Test2.o &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt; Main.o

&lt;span class=&#34;c&#34;&gt;# Here is a Make Macro defined by two Macro Expansions.
&lt;/span&gt;&lt;span class=&#34;c&#34;&gt;# A Macro Expansion may be treated as a textual replacement of the Make Macro.
&lt;/span&gt;&lt;span class=&#34;c&#34;&gt;# Macro Expansions are introduced with $ and enclosed in (parentheses).
&lt;/span&gt;&lt;span class=&#34;c&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;REBUILDABLES&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;$(&lt;/span&gt;OBJS&lt;span class=&#34;k&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;$(&lt;/span&gt;LINK_TARGET&lt;span class=&#34;k&#34;&gt;)&lt;/span&gt;

&lt;span class=&#34;c&#34;&gt;# Here is a simple Rule (used for &amp;#34;cleaning&amp;#34; your build environment).
&lt;/span&gt;&lt;span class=&#34;c&#34;&gt;# It has a Target named &amp;#34;clean&amp;#34; (left of the colon &amp;#34;:&amp;#34; on the first line),
&lt;/span&gt;&lt;span class=&#34;c&#34;&gt;# no Dependencies (right of the colon),
&lt;/span&gt;&lt;span class=&#34;c&#34;&gt;# and two Commands (indented by tabs on the lines that follow).
&lt;/span&gt;&lt;span class=&#34;c&#34;&gt;# The space before the colon is not required but added here for clarity.
&lt;/span&gt;&lt;span class=&#34;c&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;clean &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;:&lt;/span&gt; 
  rm -f &lt;span class=&#34;k&#34;&gt;$(&lt;/span&gt;REBUILDABLES&lt;span class=&#34;k&#34;&gt;)&lt;/span&gt;
  &lt;span class=&#34;nb&#34;&gt;echo&lt;/span&gt; Clean &lt;span class=&#34;k&#34;&gt;done&lt;/span&gt;

&lt;span class=&#34;c&#34;&gt;# There are two standard Targets your Makefile should probably have:
&lt;/span&gt;&lt;span class=&#34;c&#34;&gt;# &amp;#34;all&amp;#34; and &amp;#34;clean&amp;#34;, because they are often command-line Goals.
&lt;/span&gt;&lt;span class=&#34;c&#34;&gt;# Also, these are both typically Artificial Targets, because they don&amp;#39;t typically
&lt;/span&gt;&lt;span class=&#34;c&#34;&gt;# correspond to real files named &amp;#34;all&amp;#34; or &amp;#34;clean&amp;#34;.  
&lt;/span&gt;&lt;span class=&#34;c&#34;&gt;&lt;/span&gt;
&lt;span class=&#34;c&#34;&gt;# The rule for &amp;#34;all&amp;#34; is used to incrementally build your system.
&lt;/span&gt;&lt;span class=&#34;c&#34;&gt;# It does this by expressing a dependency on the results of that system,
&lt;/span&gt;&lt;span class=&#34;c&#34;&gt;# which in turn have their own rules and dependencies.
&lt;/span&gt;&lt;span class=&#34;c&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;all &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;$(&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;LINK_TARGET&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;)&lt;/span&gt;
  &lt;span class=&#34;nb&#34;&gt;echo&lt;/span&gt; All &lt;span class=&#34;k&#34;&gt;done&lt;/span&gt;

&lt;span class=&#34;c&#34;&gt;# There is no required order to the list of rules as they appear in the Makefile.
&lt;/span&gt;&lt;span class=&#34;c&#34;&gt;# Make will build its own dependency tree and only execute each rule only once
&lt;/span&gt;&lt;span class=&#34;c&#34;&gt;# its dependencies&amp;#39; rules have been executed successfully.
&lt;/span&gt;&lt;span class=&#34;c&#34;&gt;&lt;/span&gt;
&lt;span class=&#34;c&#34;&gt;# Here is a Rule that uses some built-in Make Macros in its command:
&lt;/span&gt;&lt;span class=&#34;c&#34;&gt;# $@ expands to the rule&amp;#39;s target, in this case &amp;#34;test_me.exe&amp;#34;.
&lt;/span&gt;&lt;span class=&#34;c&#34;&gt;# $^ expands to the rule&amp;#39;s dependencies, in this case the three files
&lt;/span&gt;&lt;span class=&#34;c&#34;&gt;# main.o, test1.o, and  test2.o.
&lt;/span&gt;&lt;span class=&#34;c&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;$(LINK_TARGET) &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;$(&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;OBJS&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;)&lt;/span&gt;
  g++ -g -o &lt;span class=&#34;nv&#34;&gt;$@&lt;/span&gt; $^

&lt;span class=&#34;c&#34;&gt;# Here is a Pattern Rule, often used for compile-line.
&lt;/span&gt;&lt;span class=&#34;c&#34;&gt;# It says how to create a file with a .o suffix, given a file with a .cpp suffix.
&lt;/span&gt;&lt;span class=&#34;c&#34;&gt;# The rule&amp;#39;s command uses some built-in Make Macros:
&lt;/span&gt;&lt;span class=&#34;c&#34;&gt;# $@ for the pattern-matched target
&lt;/span&gt;&lt;span class=&#34;c&#34;&gt;# $&amp;lt; for the pattern-matched dependency
&lt;/span&gt;&lt;span class=&#34;c&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;%.o &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;:&lt;/span&gt; %.&lt;span class=&#34;n&#34;&gt;cpp&lt;/span&gt;
  g++ -g -o &lt;span class=&#34;nv&#34;&gt;$@&lt;/span&gt; -c $&amp;lt;

&lt;span class=&#34;c&#34;&gt;# These are Dependency Rules, which are rules without any command.
&lt;/span&gt;&lt;span class=&#34;c&#34;&gt;# Dependency Rules indicate that if any file to the right of the colon changes,
&lt;/span&gt;&lt;span class=&#34;c&#34;&gt;# the target to the left of the colon should be considered out-of-date.
&lt;/span&gt;&lt;span class=&#34;c&#34;&gt;# The commands for making an out-of-date target up-to-date may be found elsewhere
&lt;/span&gt;&lt;span class=&#34;c&#34;&gt;# (in this case, by the Pattern Rule above).
&lt;/span&gt;&lt;span class=&#34;c&#34;&gt;# Dependency Rules are often used to capture header file dependencies.
&lt;/span&gt;&lt;span class=&#34;c&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nf&#34;&gt;Main.o &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Main&lt;/span&gt;.&lt;span class=&#34;n&#34;&gt;h&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Test&lt;/span&gt;1.&lt;span class=&#34;n&#34;&gt;h&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Test&lt;/span&gt;2.&lt;span class=&#34;n&#34;&gt;h&lt;/span&gt;
&lt;span class=&#34;nf&#34;&gt;Test1.o &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Test&lt;/span&gt;1.&lt;span class=&#34;n&#34;&gt;h&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Test&lt;/span&gt;2.&lt;span class=&#34;n&#34;&gt;h&lt;/span&gt;
&lt;span class=&#34;nf&#34;&gt;Test2.o &lt;/span&gt;&lt;span class=&#34;o&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Test&lt;/span&gt;2.&lt;span class=&#34;n&#34;&gt;h&lt;/span&gt;

&lt;span class=&#34;c&#34;&gt;# Alternatively to manually capturing dependencies, several automated
&lt;/span&gt;&lt;span class=&#34;c&#34;&gt;# dependency generators exist.  Here is one possibility (commented out)...
&lt;/span&gt;&lt;span class=&#34;c&#34;&gt;# %.dep : %.cpp
&lt;/span&gt;&lt;span class=&#34;c&#34;&gt;#   g++ -M $(FLAGS) $&amp;lt; &amp;gt; $@
&lt;/span&gt;&lt;span class=&#34;c&#34;&gt;# include $(OBJS:.o=.dep)
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;http://akaedu.github.io/book/index.html&#34;&gt;Linux C编程一站式学习&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.gnu.org/software/make/manual/make.html#Wildcards&#34;&gt;GNU make tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www3.ntu.edu.sg/home/ehchua/programming/cpp/gcc_make.html&#34;&gt;GCC and Make Compiling, Linking and BuildingC/C++ Applications&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://prateekvjoshi.com/2014/02/01/cmake-vs-make/&#34;&gt;CMake VS Make&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
</description>
                
                
                
                
                
                    
                        
                            
                                
                                
                                
                                    <category domain="https://zchencn.github.io/categories/computer-science/">Computer Science</category>
                                
                            
                        
                    
                        
                            
                                
                                
                                
                                    <category domain="https://zchencn.github.io/tags/c/">C</category>
                                
                            
                                
                                
                                
                                    <category domain="https://zchencn.github.io/tags/make-file/">Make File</category>
                                
                            
                        
                    
                
            </item>
        
    </channel>
</rss>
